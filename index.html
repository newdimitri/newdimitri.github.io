<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="深度学习 机器学习 网络安全 神经网络">
<meta property="og:type" content="website">
<meta property="og:title" content="国家二级薯条试吃员">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="国家二级薯条试吃员">
<meta property="og:description" content="深度学习 机器学习 网络安全 神经网络">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="国家二级薯条试吃员">
<meta name="twitter:description" content="深度学习 机器学习 网络安全 神经网络">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/">





  <title>国家二级薯条试吃员</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?b8223f071382d4239a383d450de56d81";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-right 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">国家二级薯条试吃员</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">没有我不敢收的红包</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      

      
    </ul>
  

  
</nav>
<script>
    
    window.onload = function(){
        var path = 'https://malizhi.cn'; //这里要改成你博客的地址
        var localhostItem = String(window.location).split(path)[1];
        var LiNode = document.querySelectorAll('#menu > li > a')
        
        for(var i = 0; i< LiNode.length;i++){
            var item = String(LiNode[i].href).split(path)[1];
            if(item == localhostItem && item != undefined){
                LiNode[i].setAttribute('style','border-bottom:1px solid black');
            }
        }
    };

</script>


 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/23/如何真正的薅AI Studio羊毛/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yimitri">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/headpic.png">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="国家二级薯条试吃员">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/23/如何真正的薅AI Studio羊毛/" itemprop="url">如何真正的薅AI Studio羊毛</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-10-23T10:45:08+08:00">
                2019-10-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>先占坑</p>
<p>Linux反弹shell（一）文件描述符与重定向</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/08/盘点 NeurIPS 两届神经网络对抗赛2017、2018/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yimitri">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/headpic.png">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="国家二级薯条试吃员">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/08/盘点 NeurIPS 两届神经网络对抗赛2017、2018/" itemprop="url">盘点 NeurIPS 两届神经网络对抗赛 (NIPS2017、NIPS2019）</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-08T12:23:48+08:00">
                2019-07-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>本文包含两大部分：<br><strong>1、NIPS2017：神经网络太好骗？清华团队如何做到打NIPS攻防赛得3冠军的</strong></p>
<p>作者：张子豪</p>
<p>来源：<a href="https://mp.weixin.qq.com/s/P2FjJNb-Qw9SBQiDP_r0Kw" target="_blank" rel="noopener">人工智能头条</a></p>
<p>文章介绍了如何用对抗样本修改图片，误导神经网络指鹿为马；对 NIPS 2017 神经网络对抗攻防赛 3 项冠军清华团队的算法模型进行了解读。文章部分内容来自 2018 CNCC 中国计算机大会—人工智能与信息安全分会场报告。</p>
<p><strong>2、NIPS2018：NIPS 2018对抗视觉挑战赛结果公布：CMU邢波团队包揽两项冠军</strong></p>
<p>作者：Wieland Brendel</p>
<p>来源：<a href="https://mp.weixin.qq.com/s/DYUsUvE3oLp1zHH02Z4rZA" target="_blank" rel="noopener">机器之心</a></p>
<p>近日，NIPS 2018 对抗视觉挑战赛结果公布。CMU 邢波团队包揽两项冠军，另一项冠军则由来自加拿大的 LIVIA 团队斩获，清华 TSAIL 团队获得「无针对性攻击」的亚军。本文介绍了这些团队的方法大纲，不过具体细节将在 12 月 7 日 9:15–10:30 举办的 NIPS Competition 研讨会上揭晓。</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2019/07/08/盘点 NeurIPS 两届神经网络对抗赛2017、2018/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/22/nlp中的Attention注意力机制+Transformer详解/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yimitri">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/headpic.png">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="国家二级薯条试吃员">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/22/nlp中的Attention注意力机制+Transformer详解/" itemprop="url">nlp中的Attention注意力机制+Transformer详解</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-22T19:45:08+08:00">
                2019-04-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文以QA形式对自然语言处理中注意力机制（Attention）进行总结，并对Transformer进行深入解析。</p>
<p><strong>目录</strong></p>
<blockquote>
<p><strong>一、Attention机制剖析</strong><br>1、为什么要引入Attention机制？<br>2、Attention机制有哪些？（怎么分类？）<br>3、Attention机制的计算流程是怎样的？<br>4、Attention机制的变种有哪些？<br>5、一种强大的Attention机制：为什么自注意力模型（self-Attention model）在长距离序列中如此强大？<br>（1）卷积或循环神经网络难道不能处理长距离序列吗？<br>（2）要解决这种短距离依赖的“局部编码”问题，从而对输入序列建立长距离依赖关系，有哪些办法呢？<br>（3）自注意力模型（self-Attention model）具体的计算流程是怎样的呢?<br><strong>二、Transformer（Attention Is All You Need）详解</strong><br>1、Transformer的整体架构是怎样的？由哪些部分组成？<br>2、Transformer Encoder 与 Transformer Decoder 有哪些不同？<br>3、Encoder-Decoder attention 与self-attention mechanism有哪些不同？<br>4、multi-head self-attention mechanism具体的计算过程是怎样的？<br>5、Transformer在GPT和Bert等词向量预训练模型中具体是怎么应用的？有什么变化？</p>
</blockquote>
<h2 id="一、Attention机制剖析"><a href="#一、Attention机制剖析" class="headerlink" title="一、Attention机制剖析"></a><strong>一、Attention机制剖析</strong></h2><p><strong>1、为什么要引入Attention机制？</strong></p>
<p>根据通用近似定理，前馈网络和循环网络都有很强的能力。但为什么还要引入注意力机制呢？</p>
<ul>
<li><strong>计算能力的限制</strong>：当要记住很多“信息“，模型就要变得更复杂，然而目前计算能力依然是限制神经网络发展的瓶颈。</li>
<li><strong>优化算法的限制</strong>：虽然局部连接、权重共享以及pooling等优化操作可以让神经网络变得简单一些，有效缓解模型复杂度和表达能力之间的矛盾；但是，如循环神经网络中的长距离以来问题，信息“记忆”能力并不高。</li>
</ul>
<p><strong>可以借助人脑处理信息过载的方式，例如Attention机制可以提高神经网络处理信息的能力。</strong></p>
<p><strong>2、Attention机制有哪些？（怎么分类？）</strong></p>
<p>当用神经网络来处理大量的输入信息时，也可以借鉴人脑的注意力机制，只 选择一些关键的信息输入进行处理，来提高神经网络的效率。按照认知神经学中的注意力，可以总体上分为两类：</p>
<ul>
<li><strong>聚焦式（focus）注意力</strong>：自上而下的有意识的注意力，主动注意——是指有预定目的、依赖任务的、主动有意识地聚焦于某一对象的注意力；</li>
<li><strong>显著性（saliency-based）注意力</strong>：自下而上的有意识的注意力，被动注意——基于显著性的注意力是由外界刺激驱动的注意，不需要主动干预，也和任务无关；可以将<strong>max-pooling和门控（gating）机制</strong>来近似地看作是自下而上的基于显著性的注意力机制。</li>
</ul>
<p>在人工神经网络中，注意力机制一般就特指聚焦式注意力。</p>
<p><strong>3、Attention机制的计算流程是怎样的？</strong></p>
<figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-54fe529ded98721f35277a5bfa79febc_b.jpg" alt=""></noscript><img src="https://pic1.zhimg.com/80/v2-54fe529ded98721f35277a5bfa79febc_hd.jpg" alt=""><figcaption>Attention机制的实质：寻址（addressing）</figcaption></figure>

<p><strong>Attention机制的实质其实就是一个寻址（addressing）的过程</strong>，如上图所示：给定一个和任务相关的查询<strong>Query</strong>向量<strong> q</strong>，通过计算与<strong>Key</strong>的注意力分布并附加在<strong>Value</strong>上，从而计算<strong>Attention Value</strong>，这个过程实际上是<strong>Attention机制缓解神经网络模型复杂度的体现</strong>：不需要将所有的N个输入信息都输入到神经网络进行计算，只需要从X中选择一些和任务相关的信息输入给神经网络。</p>
<blockquote>
<p><strong><em>注意力机制可以分为三步：一是信息输入；二是计算注意力分布α；三是根据注意力分布α 来计算输入信息的加权平均。</em></strong></p>
</blockquote>
<p><strong>step1-信息输入</strong>：用<strong>X</strong> = [x1, · · · , xN ]表示N 个输入信息；</p>
<p><strong>step2-注意力分布计算</strong>：令<strong>Key</strong>=<strong>Value</strong>=<strong>X</strong>，则可以给出注意力分布</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Calpha_i%3Dsoftmax%28s%28key_i%2Cq%29%29%3Dsoftmax%28s%28X_i%2Cq%29%29" alt="[公式]"> </p>
<p>我们将 <img src="https://www.zhihu.com/equation?tex=%5Calpha_i" alt="[公式]"> 称之为注意力分布（概率分布）， <img src="https://www.zhihu.com/equation?tex=s%28X_i%2Cq%29" alt="[公式]"> 为注意力打分机制，有几种打分机制：</p>
<figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-981a0c9ab01531c7139e4701574cb056_b.jpg" alt=""></noscript><img src="https://pic3.zhimg.com/80/v2-981a0c9ab01531c7139e4701574cb056_hd.jpg" alt=""></figure>

<p><strong>step3-信息加权平均</strong>：注意力分布 <img src="https://www.zhihu.com/equation?tex=%5Calpha_i" alt="[公式]"> 可以解释为在上下文查询<strong>q</strong>时，第i个信息受关注的程度，采用一种“软性”的信息选择机制对输入信息<strong>X</strong>进行编码为：</p>
<p><img src="https://www.zhihu.com/equation?tex=att%28q%2CX%29%3D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%7B%5Calpha_iX_i%7D" alt="[公式]"> </p>
<p>这种编码方式为<strong>软性注意力机制（soft Attention）</strong>，软性注意力机制有两种：普通模式（<strong>Key</strong>=<strong>Value</strong>=<strong>X</strong>）和键值对模式（<strong>Key！</strong>=<strong>Value</strong>）。</p>
<figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-aa371755dc73b7137149b8d2905fc4ba_b.jpg" alt=""></noscript><img src="https://pic3.zhimg.com/80/v2-aa371755dc73b7137149b8d2905fc4ba_hd.jpg" alt=""><figcaption>软性注意力机制（soft Attention）</figcaption></figure>

<p><strong>4、Attention机制的变种有哪些？</strong></p>
<p>与普通的Attention机制（上图左）相比，Attention机制有哪些变种呢？</p>
<ul>
<li><p><strong>变种1-硬性注意力：</strong>之前提到的注意力是软性注意力，其选择的信息是所有输入信息在注意力 分布下的期望。还有一种注意力是只关注到某一个位置上的信息，叫做硬性注意力（hard attention）。硬性注意力有两种实现方式：（1）一种是选取最高概率的输入信息；（2）另一种硬性注意力可以通过在注意力分布式上随机采样的方式实现。硬性注意力模型的缺点：&gt; 硬性注意力的一个缺点是基于最大采样或随机采样的方式来选择信息。因此最终的损失函数与注意力分布之间的函数关系不可导，因此无法使用在反向传播算法进行训练。为了使用反向传播算法，一般使用软性注意力来代替硬性注意力。硬性注意力需要通过强化学习来进行训练。——<a href="https://link.zhihu.com/?target=https%3A//nndl.github.io/" target="_blank" rel="noopener">《神经网络与深度学习》</a></p>
</li>
<li><p><strong>变种2-键值对注意力：</strong>即上图右边的键值对模式，此时Key！=Value，注意力函数变为：<figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-49a8acda3757ea21218b2f7ecca6e9ae_b.jpg" alt=""></noscript><img src="https://pic3.zhimg.com/80/v2-49a8acda3757ea21218b2f7ecca6e9ae_hd.jpg" alt=""></figure></p>
</li>
<li><p><strong>变种3-多头注意力：</strong>多头注意力（multi-head attention）是利用多个查询Q = [q1, · · · , qM]，来平行地计算从输入信息中选取多个信息。每个注意力关注输入信息的不同部分，然后再进行拼接：<figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-27673fff36241d6ef163c9ac1cedcce7_b.png" alt=""></noscript><img src="https://pic4.zhimg.com/80/v2-27673fff36241d6ef163c9ac1cedcce7_hd.png" alt=""></figure></p>
</li>
</ul>
<p><strong>5、一种强大的Attention机制：为什么自注意力模型（self-Attention model）在长距离序列中如此强大？</strong></p>
<p><strong>（1）卷积或循环神经网络难道不能处理长距离序列吗？</strong></p>
<p>当使用神经网络来处理一个变长的向量序列时，我们通常可以使用卷积网络或循环网络进行编码来得到一个相同长度的输出向量序列，如图所示：</p>
<figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-8b369281a66bea6920962f45660a9f0a_b.jpg" alt=""></noscript><img src="https://pic3.zhimg.com/80/v2-8b369281a66bea6920962f45660a9f0a_hd.jpg" alt=""><figcaption> 基于卷积网络和循环网络的变长序列编码</figcaption></figure>

<p>从上图可以看出，无论卷积还是循环神经网络其实都是对变长序列的一种“<strong>局部编码</strong>”：卷积神经网络显然是基于N-gram的局部编码；而对于循环神经网络，由于梯度消失等问题也只能建立短距离依赖。</p>
<p><strong>（2）要解决这种短距离依赖的“局部编码”问题，从而对输入序列建立长距离依赖关系，有哪些办法呢？</strong></p>
<blockquote>
<p>如果要建立输入序列之间的长距离依赖关系，可以使用以下两种方法：一 种方法是增加网络的层数，通过一个深层网络来获取远距离的信息交互，另一种方法是使用全连接网络。 ——<a href="https://link.zhihu.com/?target=https%3A//nndl.github.io/" target="_blank" rel="noopener">《神经网络与深度学习》</a><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-cd2d7f0961c669d983b73db4e93ccbdc_b.jpg" alt=""></noscript><img src="https://pic1.zhimg.com/80/v2-cd2d7f0961c669d983b73db4e93ccbdc_hd.jpg" alt=""><figcaption>全连接模型和自注意力模型：实线表示为可学习的权重，虚线表示动态生成的权重。</figcaption></figure></p>
</blockquote>
<p>由上图可以看出，全连接网络虽然是一种非常直接的建模远距离依赖的模型， 但是无法处理变长的输入序列。不同的输入长度，其连接权重的大小也是不同的。</p>
<p>这时我们就可以利用注意力机制来“动态”地生成不同连接的权重，这就是自注意力模型（self-attention model）。由于自注意力模型的权重是动态生成的，因此可以处理变长的信息序列。 </p>
<p>总体来说，<strong>为什么自注意力模型（self-Attention model）如此强大</strong>：<strong>利用注意力机制来“动态”地生成不同连接的权重，从而处理变长的信息序列。</strong></p>
<p><strong>（3）自注意力模型（self-Attention model）具体的计算流程是怎样的呢?</strong></p>
<p>同样，给出信息输入：用X = [x1, · · · , xN ]表示N 个输入信息；通过线性变换得到为查询向量序列，键向量序列和值向量序列：</p>
<figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-ab400406cf423842e4274527dc5a7074_b.png" alt=""></noscript><img src="https://pic1.zhimg.com/80/v2-ab400406cf423842e4274527dc5a7074_hd.png" alt=""></figure>

<p>上面的公式可以看出，<strong>self-Attention中的Q是对自身（self）输入的变换，而在传统的Attention中，Q来自于外部。</strong></p>
<figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-fcc2df696966a9c6700d1476690cff9f_b.jpg" alt=""></noscript><img src="https://pic4.zhimg.com/80/v2-fcc2df696966a9c6700d1476690cff9f_hd.jpg" alt=""><figcaption>self-Attention计算过程剖解（来自《细讲 | Attention Is All You Need 》）</figcaption></figure>

<p>注意力计算公式为：</p>
<figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-72093f153e59cfdc851e2ac1fbf5c03d_b.jpg" alt=""></noscript><img src="https://pic2.zhimg.com/80/v2-72093f153e59cfdc851e2ac1fbf5c03d_hd.jpg" alt=""></figure>

<p>自注意力模型（self-Attention model）中，通常使用缩放点积来作为注意力打分函数，输出向量序列可以写为：</p>
<figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-2f76af60c24ba75e37f2f5df8edfdb71_b.jpg" alt=""></noscript><img src="https://pic2.zhimg.com/80/v2-2f76af60c24ba75e37f2f5df8edfdb71_hd.jpg" alt=""></figure>

<h2 id="二、Transformer（Attention-Is-All-You-Need）详解"><a href="#二、Transformer（Attention-Is-All-You-Need）详解" class="headerlink" title="二、Transformer（Attention Is All You Need）详解"></a><strong>二、Transformer（Attention Is All You Need）详解</strong></h2><p>从Transformer这篇论文的题目可以看出，Transformer的核心就是Attention，这也就是为什么本文会在剖析玩Attention机制之后会引出Transformer，如果对上面的Attention机制特别是自注意力模型（self-Attention model）理解后，Transformer就很容易理解了。</p>
<p><strong>1、Transformer的整体架构是怎样的？由哪些部分组成？</strong></p>
<figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-7f8b460cd617fedc822064c4230302b0_b.jpg" alt=""></noscript><img src="https://pic1.zhimg.com/80/v2-7f8b460cd617fedc822064c4230302b0_hd.jpg" alt=""><figcaption>Transformer模型架构</figcaption></figure>

<p>Transformer其实这就是一个Seq2Seq模型，左边一个encoder把输入读进去，右边一个decoder得到输出：</p>
<figure data-size="small"><noscript><img src="https://pic4.zhimg.com/v2-846cf91009c44c6e479bada42bfc437f_b.jpg" alt=""></noscript><img src="https://pic4.zhimg.com/80/v2-846cf91009c44c6e479bada42bfc437f_hd.jpg" alt=""><figcaption>Seq2Seq模型</figcaption></figure>

<p><strong>Transformer=Transformer Encoder+Transformer Decoder</strong></p>
<p><strong>（1）Transformer Encoder（N=6层，每层包括2个sub-layers）：</strong></p>
<figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-3b97d37951078856097069778293230a_b.jpg" alt=""></noscript><img src="https://pic3.zhimg.com/80/v2-3b97d37951078856097069778293230a_hd.jpg" alt=""><figcaption>Transformer Encoder</figcaption></figure>

<ul>
<li><strong>sub-layer-1</strong>：<strong>multi-head self-attention mechanism</strong>，用来进行self-attention。</li>
<li><strong>sub-layer-2</strong>：<strong>Position-wise Feed-forward Networks</strong>，简单的全连接网络，对每个position的向量分别进行相同的操作，包括两个线性变换和一个ReLU激活输出（输入输出层的维度都为512，中间层为2048）：<figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-5236351e3efd93d567ac1fceea7716ee_b.png" alt=""></noscript><img src="https://pic3.zhimg.com/80/v2-5236351e3efd93d567ac1fceea7716ee_hd.png" alt=""></figure></li>
</ul>
<p>每个sub-layer都使用了残差网络： <img src="https://www.zhihu.com/equation?tex=LayerNorm%28X%2Bsublayer%28X%29%29" alt="[公式]"> </p>
<p><strong>（2）Transformer Decoder（N=6层，每层包括3个sub-layers）：</strong></p>
<figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-4dc71fe78c4752645de1f1ba8dd762a4_b.jpg" alt=""></noscript><img src="https://pic1.zhimg.com/80/v2-4dc71fe78c4752645de1f1ba8dd762a4_hd.jpg" alt=""><figcaption>Transformer Decoder</figcaption></figure>

<ul>
<li><strong>sub-layer-1</strong>：<strong>Masked multi-head self-attention mechanism</strong>，用来进行self-attention，与Encoder不同：由于是序列生成过程，所以在时刻 i 的时候，大于 i 的时刻都没有结果，只有小于 i 的时刻有结果，因此需要做<strong>Mask</strong>。</li>
<li><strong>sub-layer-2</strong>：<strong>Position-wise Feed-forward Networks</strong>，同Encoder。</li>
<li><strong>sub-layer-3</strong>：<strong>Encoder-Decoder attention计算</strong>。</li>
</ul>
<p><strong>2、Transformer Encoder 与 Transformer Decoder 有哪些不同？</strong></p>
<p>（1）multi-head self-attention mechanism不同，Encoder中不需要使用Masked，而Decoder中需要使用Masked；</p>
<p>（2）Decoder中多了一层Encoder-Decoder attention，这与 self-attention mechanism不同。</p>
<p><strong>3、Encoder-Decoder attention 与self-attention mechanism有哪些不同？</strong></p>
<p>它们都是用了 multi-head计算，不过Encoder-Decoder attention采用传统的attention机制，其中的Query是self-attention mechanism已经计算出的上一时间i处的编码值，Key和Value都是Encoder的输出，这与self-attention mechanism不同。代码中具体体现：</p>
<div class="highlight"><br><br>     ## Multihead Attention ( self-attention)<br>                self.dec = multihead_attention(queries=self.dec,<br>                                               keys=self.dec,<br>                                               num_units=hp.hidden_units,<br>                                               num_heads=hp.num_heads,<br>                                               dropout_rate=hp.dropout_rate,<br>                                               is_training=is_training,<br>                                               causality=True,<br>                                               scope=”self_attention”)<br><br>    ## Multihead Attention ( Encoder-Decoder attention)<br>                self.dec = multihead_attention(queries=self.dec,<br>                                               keys=self.enc,<br>                                               num_units=hp.hidden_units,<br>                                               num_heads=hp.num_heads,<br>                                               dropout_rate=hp.dropout_rate,<br>                                               is_training=is_training,<br>                                               causality=False,<br>                                               scope=”vanilla_attention”)<br></div>

<p><strong>4、multi-head self-attention mechanism具体的计算过程是怎样的？</strong> </p>
<figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-392692c19c57f5bfa116f7b505dfde7a_b.jpg" alt=""></noscript><img src="https://pic3.zhimg.com/80/v2-392692c19c57f5bfa116f7b505dfde7a_hd.jpg" alt=""><figcaption>multi-head self-attention mechanism计算过程</figcaption></figure>

<p>Transformer中的Attention机制由<strong>Scaled Dot-Product Attention</strong>和<strong>Multi-Head Attention</strong>组成，上图给出了整体流程。下面具体介绍各个环节：</p>
<ul>
<li><strong>Expand</strong>：实际上是经过线性变换，生成Q、K、V三个向量；</li>
<li><strong>Split heads</strong>: 进行分头操作，在原文中将原来每个位置512维度分成8个head，每个head维度变为64；</li>
<li><strong>Self Attention</strong>：对每个head进行Self Attention，具体过程和第一部分介绍的一致；</li>
<li><strong>Concat heads</strong>：对进行完Self Attention每个head进行拼接；</li>
</ul>
<p>上述过程公式为：</p>
<figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-c7100e268bcefaa7ff0a1344acc15e7e_b.jpg" alt=""></noscript><img src="https://pic3.zhimg.com/80/v2-c7100e268bcefaa7ff0a1344acc15e7e_hd.jpg" alt=""></figure>

<p><strong>5、Transformer在GPT和Bert等词向量预训练模型中具体是怎么应用的？有什么变化？</strong></p>
<ul>
<li>GPT中训练的是单向语言模型，其实就是直接应用<strong>Transformer Decoder</strong>；</li>
<li>Bert中训练的是双向语言模型，应用了<strong>Transformer Encoder</strong>部分，不过在Encoder基础上还做了<strong>Masked操作</strong>；</li>
</ul>
<p>BERT Transformer 使用双向self-attention，而GPT Transformer 使用受限制的self-attention，其中每个token只能处理其左侧的上下文。双向 Transformer 通常被称为“Transformer encoder”，而左侧上下文被称为“Transformer decoder”，decoder是不能获要预测的信息的。</p>
<p><strong>Reference</strong></p>
<ol>
<li><a href="https://link.zhihu.com/?target=https%3A//nndl.github.io/" target="_blank" rel="noopener">《神经网络与深度学习》</a>2.  <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a>3.  <a href="https://link.zhihu.com/?target=http%3A//www.chinahadoop.cn/course/1253" target="_blank" rel="noopener">谷歌BERT解析—-2小时上手最强NLP训练模型</a>4.  <a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/RLxWevVWHXgX-UcoxDS70w" target="_blank" rel="noopener">细讲 | Attention Is All You Need</a>5.  <a href="https://zhuanlan.zhihu.com/p/37601161" target="_blank" rel="noopener">深度学习中的注意力模型（2017版）</a></li>
</ol>
<p>tt</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/13/啥是Apache Flink！/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yimitri">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/headpic.png">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="国家二级薯条试吃员">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/13/啥是Apache Flink！/" itemprop="url">啥是Apache Flink！</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-13T22:16:08+08:00">
                2019-04-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>Apache Flink（以下简称Flink）项目是大数据处理领域最近冉冉升起的一颗新星，其不同于其他大数据项目的诸多特性吸引了越来越多人的关注。本文将深入分析Flink的一些关键技术与特性，希望能够帮助读者对Flink有更加深入的了解，对其他大数据系统开发者也能有所裨益。本文假设读者已对MapReduce、Spark及Storm等大数据处理框架有所了解，同时熟悉流处理与批处理的基本概念。</p>
<h1 id="Flink简介"><a href="#Flink简介" class="headerlink" title="Flink简介"></a>Flink简介</h1><p>Flink核心是一个流式的数据流执行引擎，其针对数据流的分布式计算提供了数据分布、数据通信以及容错机制等功能。基于流执行引擎，Flink提供了诸多更高抽象层的API以便用户编写分布式任务：<br></p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2019/04/13/啥是Apache Flink！/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/13/为什么业界很少使用 Haskell？/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yimitri">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/headpic.png">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="国家二级薯条试吃员">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/13/为什么业界很少使用 Haskell？/" itemprop="url">为什么业界很少使用 Haskell？</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-13T22:15:08+08:00">
                2019-04-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>这是 Stackoverflow 中一篇答案的粗略翻译，原文地址 <a href="https://stackoverflow.com/a/2302230/296473" target="_blank" rel="noopener">http://stackoverflow.com/a/2302230/296473</a>已失效。</p>
<ol>
<li><p><strong>没有人听说过它。</strong>没有人会使用他们根本不知道的东西。</p>
</li>
<li><p><strong>不够流行。</strong>人们认为最流行的语言就是最好的语言，因为如果它不好的话，它就不会流行。实际上这根本不成立。最流行的语言最流行，仅此而已。Haskell 不流行是因为它不流行。这就是 Haskell 里经常用到的「递归」。不管来自命令式编程世界的人们怎么说，递归在现实世界中非常常见。</p>
</li>
<li><p><strong>它不一样。</strong>人们总是害怕新事物。</p></li></ol>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2019/04/13/为什么业界很少使用 Haskell？/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/26/图解BERT/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yimitri">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/headpic.png">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="国家二级薯条试吃员">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/26/图解BERT/" itemprop="url">图解BERT</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-26T22:15:08+08:00">
                2019-03-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h2 id="图解BERT（NLP中的迁移学习）"><a href="#图解BERT（NLP中的迁移学习）" class="headerlink" title="图解BERT（NLP中的迁移学习）"></a><a href="">图解BERT（NLP中的迁移学习）</a></h2><p>目录</p>
<ul>
<li><a href="#一例子句子分类">一、例子:句子分类</a></li>
<li><a href="#二模型架构">二、模型架构</a><ul>
<li><a href="#模型的输入">模型的输入</a></li>
<li><a href="#模型的输出">模型的输出</a></li>
</ul>
</li>
<li><a href="#三与卷积网络并行">三、与卷积网络并行</a></li>
<li><a href="#四嵌入表示的新时代">四、嵌入表示的新时代</a><ul>
<li><a href="#回顾一下词嵌入">回顾一下词嵌入</a></li>
<li><a href="#elmo-语境的重要性">ELMo: 语境的重要性</a></li>
</ul>
</li>
<li><a href="#五ulm-fit搞懂nlp中的迁移学习">五、ULM-FiT：搞懂NLP中的迁移学习</a></li>
<li><a href="#六transformer超越lstm">六、Transformer：超越LSTM</a></li>
<li><a href="#七openaitransformer为语言建模预训练一个transformer解码器">七、OpenAI　Transformer：为语言建模预训练一个Transformer解码器</a></li>
<li><a href="#八在下游任务中使用迁移学习">八、在下游任务中使用迁移学习</a></li>
<li><a href="#九bert从解码器到编码器">九、BERT：从解码器到编码器</a><ul>
<li><a href="#mlm语言模型">MLM语言模型</a></li>
<li><a href="#两个句子的任务">两个句子的任务</a></li>
<li><a href="#解决特定任务的模型">解决特定任务的模型</a></li>
<li><a href="#用于特征提取的bert">用于特征提取的BERT</a></li>
</ul>
</li>
<li><a href="#十把bert牵出来遛一遛">十、把BERT牵出来遛一遛</a></li>
</ul>
<p><em>本文翻译自Jay Alammar的博客<a href="https://jalammar.github.io/illustrated-bert/" target="_blank" rel="noopener">The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)</a></em><br></p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2019/03/26/图解BERT/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/25/BERT模型评价/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yimitri">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/headpic.png">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="国家二级薯条试吃员">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/25/BERT模型评价/" itemprop="url">BERT模型评价</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-25T22:15:08+08:00">
                2019-03-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p><strong>深度神经网络的超强有效性一直让人疑惑。</strong></p>
<p>经典论文《可视化与理解CNN》（Visualizing and Understanding Convolutional Networks）解释了在图像领域中CNN从低层到高层不断学习出图像的边缘、转角、组合、局部、整体信息的过程，一定层面论证了深度学习的有效性。<strong>另一方面，传统的NLP神经网络却并不是那么深，而bert的出现直接将NLP的神经网络加到12层以上。</strong></p>
<p><strong>那么如何理解各层学到了怎样的信息？</strong><br></p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2019/03/25/BERT模型评价/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/24/BERT算法原理解析/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yimitri">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/headpic.png">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="国家二级薯条试吃员">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/24/BERT算法原理解析/" itemprop="url">BERT算法原理解析</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-24T22:15:08+08:00">
                2019-03-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h1><p>谷歌在2017年发表了一篇论文名字教Attention Is All You Need，<strong>提出了一个只基于attention的结构来处理序列模型</strong>相关的问题，比如机器翻译。传统的神经机器翻译大都是利用RNN或者CNN来作为encoder-decoder的模型基础，而谷歌最新的只基于Attention的Transformer模型摒弃了固有的定式，并没有用任何CNN或者RNN的结构。该模型可以高度并行地工作，所以在提升翻译性能的同时训练速度也特别快。</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2019/03/24/BERT算法原理解析/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/23/BERT大火却不懂Transformer？读这一篇就够了/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yimitri">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/headpic.png">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="国家二级薯条试吃员">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/23/BERT大火却不懂Transformer？读这一篇就够了/" itemprop="url">BERT大火却不懂Transformer？读这一篇就够了</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-23T22:15:08+08:00">
                2019-03-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>大数据文摘与百度NLP联合出品</p>
<p>前一段时间谷歌推出的BERT模型在11项NLP任务中夺得STOA结果，引爆了整个NLP界。而BERT取得成功的一个关键因素是Transformer的强大作用。谷歌的Transformer模型最早是用于机器翻译任务，当时达到了STOA效果。Transformer改进了RNN最被人诟病的训练慢的缺点，利用self-attention机制实现快速并行。并且Transformer可以增加到非常深的深度，充分发掘DNN模型的特性，提升模型准确率。在本文中，我们将研究Transformer模型，把它掰开揉碎，理解它的工作原理。<br></p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2019/03/23/BERT大火却不懂Transformer？读这一篇就够了/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/23/图解Transformer/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yimitri">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/headpic.png">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="国家二级薯条试吃员">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/23/图解Transformer/" itemprop="url">图解Transformer</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-23T22:15:08+08:00">
                2019-03-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><pre><code> Attention这种机制最开始应用于机器翻译的任务中，并且取得了巨大的成就，因而在最近的深度学习模型中受到了大量的关注。在在这个基础上，我们提出一种完全基于Attention机制来加速深度学习训练过程的算法模型-Transformer。事实证明Transformer结构在特定任务上已经优于了谷歌的神经网络机器翻译模型。但是，Transformer最大的优势在于其在并行化处理上做出的贡献。谷歌也在利用Transformer的并行化方式来营销自己的云TPU。所以，现在让我们一步一步剖析Transformer的神秘面纱，让我看看他是怎么一步一步训练的。

Transformer在Goole的一篇论文[Attention is All You Need](https://arxiv.org/abs/1706.03762)被提出，为了方便实现调用Transformer Google还开源了一个第三方库，基于TensorFlow的[Tensor2Tensor](https://github.com/tensorflow/tensor2tensor)，一个NLP的社区研究者贡献了一个Torch版本的支持：[guide annotating the paper with PyTorch implementation](http://nlp.seas.harvard.edu/2018/04/03/attention.html)。这里，我想用一些方便理解的方式来一步一步解释Transformer的训练过程，这样即便你没有很深的深度学习知识你也能大概明白其中的原理。
</code></pre>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2019/03/23/图解Transformer/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/headpic.png" alt="Yimitri">
            
              <p class="site-author-name" itemprop="name">Yimitri</p>
              <p class="site-description motion-element" itemprop="description">深度学习 机器学习 网络安全 神经网络</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">16</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">分类</span>
                
              </div>
            

            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/newdimitri" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:Yimitrii@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://ws3.sinaimg.cn/large/005BYqpgly1g1jru8map2j30kw0r241d.jpg" target="_blank" title="Wechat">
                      
                        <i class="fa fa-fw fa-weixin"></i>Wechat</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.zhihu.com/org/peddlepaddleqian-yan-gen-zong/activities" target="_blank" title="知乎">
                      
                        <i class="fa fa-fw fa-mortar-board (alias)"></i>知乎</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yimitri</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>



<!-- 新增访客统计代码 -->

<div class="busuanzi-count">
    <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="site-uv">
      <i class="fa fa-user"></i>
      访问用户： <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 人
    </span>
    <div class="powered-by"></div>
    <span class="site-uv">
      <i class="fa fa-eye"></i>
      访问次数： <span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 次
    </span>
    <!-- 博客字数统计 -->
    <span class="site-pv">
      <i class="fa fa-pencil"></i>
      博客全站共： <span class="post-count"></span> 字
    </span>
</div>
<!-- 新增访客统计代码 END-->
        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  

  

  

</body>
</html>
