<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="本文以QA形式对自然语言处理中注意力机制（Attention）进行总结，并对Transformer进行深入解析。 目录  一、Attention机制剖析1、为什么要引入Attention机制？2、Attention机制有哪些？（怎么分类？）3、Attention机制的计算流程是怎样的？4、Attention机制的变种有哪些？5、一种强大的Attention机制：为什么自注意力模型（self-Att">
<meta property="og:type" content="article">
<meta property="og:title" content="nlp中的Attention注意力机制+Transformer详解">
<meta property="og:url" content="http://yoursite.com/2019/04/22/nlp中的Attention注意力机制+Transformer详解/index.html">
<meta property="og:site_name" content="没有我不敢收的红包">
<meta property="og:description" content="本文以QA形式对自然语言处理中注意力机制（Attention）进行总结，并对Transformer进行深入解析。 目录  一、Attention机制剖析1、为什么要引入Attention机制？2、Attention机制有哪些？（怎么分类？）3、Attention机制的计算流程是怎样的？4、Attention机制的变种有哪些？5、一种强大的Attention机制：为什么自注意力模型（self-Att">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://pic1.zhimg.com/v2-54fe529ded98721f35277a5bfa79febc\_b.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Calpha_i%3Dsoftmax%28s%28key_i%2Cq%29%29%3Dsoftmax%28s%28X_i%2Cq%29%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Calpha_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=s%28X_i%2Cq%29">
<meta property="og:image" content="https://pic3.zhimg.com/v2-981a0c9ab01531c7139e4701574cb056\_b.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Calpha_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=att%28q%2CX%29%3D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%7B%5Calpha_iX_i%7D">
<meta property="og:image" content="https://pic3.zhimg.com/v2-aa371755dc73b7137149b8d2905fc4ba\_b.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/v2-49a8acda3757ea21218b2f7ecca6e9ae\_b.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/v2-27673fff36241d6ef163c9ac1cedcce7\_b.png">
<meta property="og:image" content="https://pic3.zhimg.com/v2-8b369281a66bea6920962f45660a9f0a\_b.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/v2-cd2d7f0961c669d983b73db4e93ccbdc\_b.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/v2-ab400406cf423842e4274527dc5a7074\_b.png">
<meta property="og:image" content="https://pic4.zhimg.com/v2-fcc2df696966a9c6700d1476690cff9f\_b.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/v2-72093f153e59cfdc851e2ac1fbf5c03d\_b.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/v2-2f76af60c24ba75e37f2f5df8edfdb71\_b.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/v2-7f8b460cd617fedc822064c4230302b0\_b.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/v2-846cf91009c44c6e479bada42bfc437f\_b.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/v2-3b97d37951078856097069778293230a\_b.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/v2-5236351e3efd93d567ac1fceea7716ee\_b.png">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=LayerNorm%28X%2Bsublayer%28X%29%29">
<meta property="og:image" content="https://pic1.zhimg.com/v2-4dc71fe78c4752645de1f1ba8dd762a4\_b.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/v2-392692c19c57f5bfa116f7b505dfde7a\_b.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/v2-c7100e268bcefaa7ff0a1344acc15e7e\_b.jpg">
<meta property="og:updated_time" content="2019-04-22T11:39:30.415Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="nlp中的Attention注意力机制+Transformer详解">
<meta name="twitter:description" content="本文以QA形式对自然语言处理中注意力机制（Attention）进行总结，并对Transformer进行深入解析。 目录  一、Attention机制剖析1、为什么要引入Attention机制？2、Attention机制有哪些？（怎么分类？）3、Attention机制的计算流程是怎样的？4、Attention机制的变种有哪些？5、一种强大的Attention机制：为什么自注意力模型（self-Att">
<meta name="twitter:image" content="https://pic1.zhimg.com/v2-54fe529ded98721f35277a5bfa79febc\_b.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/04/22/nlp中的Attention注意力机制+Transformer详解/">





  <title>nlp中的Attention注意力机制+Transformer详解 | 没有我不敢收的红包</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?b8223f071382d4239a383d450de56d81";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">没有我不敢收的红包</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">国家二级薯条试吃员</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      

      
    </ul>
  

  
</nav>
<script>
    
    window.onload = function(){
        var path = 'https://malizhi.cn'; //这里要改成你博客的地址
        var localhostItem = String(window.location).split(path)[1];
        var LiNode = document.querySelectorAll('#menu > li > a')
        
        for(var i = 0; i< LiNode.length;i++){
            var item = String(LiNode[i].href).split(path)[1];
            if(item == localhostItem && item != undefined){
                LiNode[i].setAttribute('style','border-bottom:1px solid black');
            }
        }
    };

</script>


 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/22/nlp中的Attention注意力机制+Transformer详解/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yimitri">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/headpic.png">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="没有我不敢收的红包">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">nlp中的Attention注意力机制+Transformer详解</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-22T19:45:08+08:00">
                2019-04-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本文以QA形式对自然语言处理中注意力机制（Attention）进行总结，并对Transformer进行深入解析。</p>
<p><strong>目录</strong></p>
<blockquote>
<p><strong>一、Attention机制剖析</strong><br>1、为什么要引入Attention机制？<br>2、Attention机制有哪些？（怎么分类？）<br>3、Attention机制的计算流程是怎样的？<br>4、Attention机制的变种有哪些？<br>5、一种强大的Attention机制：为什么自注意力模型（self-Attention model）在长距离序列中如此强大？<br>（1）卷积或循环神经网络难道不能处理长距离序列吗？<br>（2）要解决这种短距离依赖的“局部编码”问题，从而对输入序列建立长距离依赖关系，有哪些办法呢？<br>（3）自注意力模型（self-Attention model）具体的计算流程是怎样的呢?<br><strong>二、Transformer（Attention Is All You Need）详解</strong><br>1、Transformer的整体架构是怎样的？由哪些部分组成？<br>2、Transformer Encoder 与 Transformer Decoder 有哪些不同？<br>3、Encoder-Decoder attention 与self-attention mechanism有哪些不同？<br>4、multi-head self-attention mechanism具体的计算过程是怎样的？<br>5、Transformer在GPT和Bert等词向量预训练模型中具体是怎么应用的？有什么变化？<br><a id="more"></a>  </p>
</blockquote>
<h2 id="一、Attention机制剖析"><a href="#一、Attention机制剖析" class="headerlink" title="一、Attention机制剖析"></a><strong>一、Attention机制剖析</strong></h2><p><strong>1、为什么要引入Attention机制？</strong></p>
<p>根据通用近似定理，前馈网络和循环网络都有很强的能力。但为什么还要引入注意力机制呢？</p>
<ul>
<li><strong>计算能力的限制</strong>：当要记住很多“信息“，模型就要变得更复杂，然而目前计算能力依然是限制神经网络发展的瓶颈。</li>
<li><strong>优化算法的限制</strong>：虽然局部连接、权重共享以及pooling等优化操作可以让神经网络变得简单一些，有效缓解模型复杂度和表达能力之间的矛盾；但是，如循环神经网络中的长距离以来问题，信息“记忆”能力并不高。</li>
</ul>
<p><strong>可以借助人脑处理信息过载的方式，例如Attention机制可以提高神经网络处理信息的能力。</strong></p>
<p><strong>2、Attention机制有哪些？（怎么分类？）</strong></p>
<p>当用神经网络来处理大量的输入信息时，也可以借鉴人脑的注意力机制，只 选择一些关键的信息输入进行处理，来提高神经网络的效率。按照认知神经学中的注意力，可以总体上分为两类：</p>
<ul>
<li><strong>聚焦式（focus）注意力</strong>：自上而下的有意识的注意力，主动注意——是指有预定目的、依赖任务的、主动有意识地聚焦于某一对象的注意力；</li>
<li><strong>显著性（saliency-based）注意力</strong>：自下而上的有意识的注意力，被动注意——基于显著性的注意力是由外界刺激驱动的注意，不需要主动干预，也和任务无关；可以将<strong>max-pooling和门控（gating）机制</strong>来近似地看作是自下而上的基于显著性的注意力机制。</li>
</ul>
<p>在人工神经网络中，注意力机制一般就特指聚焦式注意力。</p>
<p><strong>3、Attention机制的计算流程是怎样的？</strong></p>
<p><img src="https://pic1.zhimg.com/v2-54fe529ded98721f35277a5bfa79febc\_b.jpg" data-size="normal" data-rawwidth="640" data-rawheight="274" class="origin\_image zh-lightbox-thumb" width="640" data-original="https://pic1.zhimg.com/v2-54fe529ded98721f35277a5bfa79febc_r.jpg"></p>
<p>Attention机制的实质：寻址（addressing）</p>
<p><strong>Attention机制的实质其实就是一个寻址（addressing）的过程</strong>，如上图所示：给定一个和任务相关的查询<strong>Query</strong>向量 <strong>q</strong>，通过计算与<strong>Key</strong>的注意力分布并附加在<strong>Value</strong>上，从而计算<strong>Attention Value</strong>，这个过程实际上是<strong>Attention机制缓解神经网络模型复杂度的体现</strong>：不需要将所有的N个输入信息都输入到神经网络进行计算，只需要从X中选择一些和任务相关的信息输入给神经网络。</p>
<blockquote>
<p><strong><em>注意力机制可以分为三步：一是信息输入；二是计算注意力分布α；三是根据注意力分布α 来计算输入信息的加权平均。</em></strong></p>
</blockquote>
<p><strong>step1-信息输入</strong>：用<strong>X</strong> = [x1, · · · , xN ]表示N 个输入信息；</p>
<p><strong>step2-注意力分布计算</strong>：令<strong>Key</strong>=<strong>Value</strong>=<strong>X</strong>，则可以给出注意力分布</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Calpha_i%3Dsoftmax%28s%28key_i%2Cq%29%29%3Dsoftmax%28s%28X_i%2Cq%29%29" alt="\alpha_i=softmax(s(key_i,q))=softmax(s(X_i,q))"></p>
<p>我们将 <img src="https://www.zhihu.com/equation?tex=%5Calpha_i" alt="\alpha_i"> 称之为注意力分布（概率分布）， <img src="https://www.zhihu.com/equation?tex=s%28X_i%2Cq%29" alt="s(X_i,q)"> 为注意力打分机制，有几种打分机制：</p>
<p><img src="https://pic3.zhimg.com/v2-981a0c9ab01531c7139e4701574cb056\_b.jpg" data-caption="" data-size="normal" data-rawwidth="434" data-rawheight="141" class="origin\_image zh-lightbox-thumb" width="434" data-original="https://pic3.zhimg.com/v2-981a0c9ab01531c7139e4701574cb056_r.jpg"></p>
<p><strong>step3-信息加权平均</strong>：注意力分布 <img src="https://www.zhihu.com/equation?tex=%5Calpha_i" alt="\alpha_i"> 可以解释为在上下文查询<strong>q</strong>时，第i个信息受关注的程度，采用一种“软性”的信息选择机制对输入信息<strong>X</strong>进行编码为：</p>
<p><img src="https://www.zhihu.com/equation?tex=att%28q%2CX%29%3D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%7B%5Calpha_iX_i%7D" alt="att(q,X)=\sum_{i=1}^{N}{\alpha_iX_i}"></p>
<p>这种编码方式为<strong>软性注意力机制（soft Attention）</strong>，软性注意力机制有两种：普通模式（<strong>Key</strong>=<strong>Value</strong>=<strong>X</strong>）和键值对模式（<strong>Key！</strong>=<strong>Value</strong>）。</p>
<p><img src="https://pic3.zhimg.com/v2-aa371755dc73b7137149b8d2905fc4ba\_b.jpg" data-size="normal" data-rawwidth="588" data-rawheight="320" class="origin\_image zh-lightbox-thumb" width="588" data-original="https://pic3.zhimg.com/v2-aa371755dc73b7137149b8d2905fc4ba_r.jpg"></p>
<p>软性注意力机制（soft Attention）</p>
<p><strong>4、Attention机制的变种有哪些？</strong></p>
<p>与普通的Attention机制（上图左）相比，Attention机制有哪些变种呢？</p>
<ul>
<li><strong>变种1-硬性注意力：</strong>之前提到的注意力是软性注意力，其选择的信息是所有输入信息在注意力 分布下的期望。还有一种注意力是只关注到某一个位置上的信息，叫做硬性注意力（hard attention）。硬性注意力有两种实现方式：（1）一种是选取最高概率的输入信息；（2）另一种硬性注意力可以通过在注意力分布式上随机采样的方式实现。硬性注意力模型的缺点：</li>
</ul>
<blockquote>
<p>硬性注意力的一个缺点是基于最大采样或随机采样的方式来选择信息。因此最终的损失函数与注意力分布之间的函数关系不可导，因此无法使用在反向传播算法进行训练。为了使用反向传播算法，一般使用软性注意力来代替硬性注意力。硬性注意力需要通过强化学习来进行训练。——<a href="https://link.zhihu.com/?target=https%3A//nndl.github.io/" target="_blank" rel="noopener">《神经网络与深度学习》</a></p>
</blockquote>
<ul>
<li><strong>变种2-键值对注意力：</strong>即上图右边的键值对模式，此时Key！=Value，注意力函数变为：</li>
</ul>
<p><img src="https://pic3.zhimg.com/v2-49a8acda3757ea21218b2f7ecca6e9ae\_b.jpg" data-caption="" data-size="normal" data-rawwidth="332" data-rawheight="142" class="content\_image" width="332"></p>
<ul>
<li><strong>变种3-多头注意力：</strong>多头注意力（multi-head attention）是利用多个查询Q = [q1, · · · , qM]，来平行地计算从输入信息中选取多个信息。每个注意力关注输入信息的不同部分，然后再进行拼接：</li>
</ul>
<p><img src="https://pic4.zhimg.com/v2-27673fff36241d6ef163c9ac1cedcce7\_b.png" data-caption="" data-size="normal" data-rawwidth="472" data-rawheight="42" class="origin\_image zh-lightbox-thumb" width="472" data-original="https://pic4.zhimg.com/v2-27673fff36241d6ef163c9ac1cedcce7_r.jpg"></p>
<p><strong>5、一种强大的Attention机制：为什么自注意力模型（self-Attention model）在长距离序列中如此强大？</strong></p>
<p><strong>（1）卷积或循环神经网络难道不能处理长距离序列吗？</strong></p>
<p>当使用神经网络来处理一个变长的向量序列时，我们通常可以使用卷积网络或循环网络进行编码来得到一个相同长度的输出向量序列，如图所示：</p>
<p><img src="https://pic3.zhimg.com/v2-8b369281a66bea6920962f45660a9f0a\_b.jpg" data-size="normal" data-rawwidth="569" data-rawheight="137" class="origin\_image zh-lightbox-thumb" width="569" data-original="https://pic3.zhimg.com/v2-8b369281a66bea6920962f45660a9f0a_r.jpg"></p>
<p>基于卷积网络和循环网络的变长序列编码</p>
<p>从上图可以看出，无论卷积还是循环神经网络其实都是对变长序列的一种“<strong>局部编码</strong>”：卷积神经网络显然是基于N-gram的局部编码；而对于循环神经网络，由于梯度消失等问题也只能建立短距离依赖。</p>
<p><strong>（2）要解决这种短距离依赖的“局部编码”问题，从而对输入序列建立长距离依赖关系，有哪些办法呢？</strong></p>
<blockquote>
<p>如果要建立输入序列之间的长距离依赖关系，可以使用以下两种方法：一 种方法是增加网络的层数，通过一个深层网络来获取远距离的信息交互，另一种方法是使用全连接网络。 ——<a href="https://link.zhihu.com/?target=https%3A//nndl.github.io/" target="_blank" rel="noopener">《神经网络与深度学习》</a></p>
</blockquote>
<p><img src="https://pic1.zhimg.com/v2-cd2d7f0961c669d983b73db4e93ccbdc\_b.jpg" data-size="normal" data-rawwidth="588" data-rawheight="167" class="origin\_image zh-lightbox-thumb" width="588" data-original="https://pic1.zhimg.com/v2-cd2d7f0961c669d983b73db4e93ccbdc_r.jpg"></p>
<p>全连接模型和自注意力模型：实线表示为可学习的权重，虚线表示动态生成的权重。</p>
<p>由上图可以看出，全连接网络虽然是一种非常直接的建模远距离依赖的模型， 但是无法处理变长的输入序列。不同的输入长度，其连接权重的大小也是不同的。</p>
<p>这时我们就可以利用注意力机制来“动态”地生成不同连接的权重，这就是自注意力模型（self-attention model）。由于自注意力模型的权重是动态生成的，因此可以处理变长的信息序列。</p>
<p>总体来说，<strong>为什么自注意力模型（self-Attention model）如此强大</strong>：<strong>利用注意力机制来“动态”地生成不同连接的权重，从而处理变长的信息序列。</strong></p>
<p><strong>（3）自注意力模型（self-Attention model）具体的计算流程是怎样的呢?</strong></p>
<p>同样，给出信息输入：用X = [x1, · · · , xN ]表示N 个输入信息；通过线性变换得到为查询向量序列，键向量序列和值向量序列：</p>
<p><img src="https://pic1.zhimg.com/v2-ab400406cf423842e4274527dc5a7074\_b.png" data-caption="" data-size="normal" data-rawwidth="94" data-rawheight="108" class="content\_image" width="94"></p>
<p>上面的公式可以看出，<strong>self-Attention中的Q是对自身（self）输入的变换，而在传统的Attention中，Q来自于外部。</strong></p>
<p><img src="https://pic4.zhimg.com/v2-fcc2df696966a9c6700d1476690cff9f\_b.jpg" data-size="normal" data-rawwidth="905" data-rawheight="700" class="origin\_image zh-lightbox-thumb" width="905" data-original="https://pic4.zhimg.com/v2-fcc2df696966a9c6700d1476690cff9f_r.jpg"></p>
<p>self-Attention计算过程剖解（来自《细讲 | Attention Is All You Need 》）</p>
<p>注意力计算公式为：</p>
<p><img src="https://pic2.zhimg.com/v2-72093f153e59cfdc851e2ac1fbf5c03d\_b.jpg" data-caption="" data-size="normal" data-rawwidth="260" data-rawheight="180" class="content\_image" width="260"></p>
<p>自注意力模型（self-Attention model）中，通常使用缩放点积来作为注意力打分函数，输出向量序列可以写为：</p>
<p><img src="https://pic2.zhimg.com/v2-2f76af60c24ba75e37f2f5df8edfdb71\_b.jpg" data-caption="" data-size="normal" data-rawwidth="210" data-rawheight="52" class="content\_image" width="210"></p>
<h2 id="二、Transformer（Attention-Is-All-You-Need）详解"><a href="#二、Transformer（Attention-Is-All-You-Need）详解" class="headerlink" title="二、Transformer（Attention Is All You Need）详解"></a><strong>二、Transformer（Attention Is All You Need）详解</strong></h2><p>从Transformer这篇论文的题目可以看出，Transformer的核心就是Attention，这也就是为什么本文会在剖析玩Attention机制之后会引出Transformer，如果对上面的Attention机制特别是自注意力模型（self-Attention model）理解后，Transformer就很容易理解了。</p>
<p><strong>1、Transformer的整体架构是怎样的？由哪些部分组成？</strong></p>
<p><img src="https://pic1.zhimg.com/v2-7f8b460cd617fedc822064c4230302b0\_b.jpg" data-size="normal" data-rawwidth="376" data-rawheight="543" class="content\_image" width="376"></p>
<p>Transformer模型架构</p>
<p>Transformer其实这就是一个Seq2Seq模型，左边一个encoder把输入读进去，右边一个decoder得到输出：</p>
<p><img src="https://pic4.zhimg.com/v2-846cf91009c44c6e479bada42bfc437f\_b.jpg" data-size="small" data-rawwidth="609" data-rawheight="415" class="origin\_image zh-lightbox-thumb" width="609" data-original="https://pic4.zhimg.com/v2-846cf91009c44c6e479bada42bfc437f_r.jpg"></p>
<p>Seq2Seq模型</p>
<p><strong>Transformer=Transformer Encoder+Transformer Decoder</strong></p>
<p><strong>（1）Transformer Encoder（N=6层，每层包括2个sub-layers）：</strong></p>
<p><img src="https://pic3.zhimg.com/v2-3b97d37951078856097069778293230a\_b.jpg" data-size="normal" data-rawwidth="1000" data-rawheight="949" class="origin\_image zh-lightbox-thumb" width="1000" data-original="https://pic3.zhimg.com/v2-3b97d37951078856097069778293230a_r.jpg"></p>
<p>Transformer Encoder</p>
<ul>
<li><strong>sub-layer-1</strong>：<strong>multi-head self-attention mechanism</strong>，用来进行self-attention。</li>
<li><strong>sub-layer-2</strong>：<strong>Position-wise Feed-forward Networks</strong>，简单的全连接网络，对每个position的向量分别进行相同的操作，包括两个线性变换和一个ReLU激活输出（输入输出层的维度都为512，中间层为2048）：</li>
</ul>
<p><img src="https://pic3.zhimg.com/v2-5236351e3efd93d567ac1fceea7716ee\_b.png" data-caption="" data-size="normal" data-rawwidth="270" data-rawheight="35" class="content\_image" width="270"></p>
<p>每个sub-layer都使用了残差网络： <img src="https://www.zhihu.com/equation?tex=LayerNorm%28X%2Bsublayer%28X%29%29" alt="LayerNorm(X+sublayer(X))"></p>
<p><strong>（2）Transformer Decoder（N=6层，每层包括3个sub-layers）：</strong></p>
<p><img src="https://pic1.zhimg.com/v2-4dc71fe78c4752645de1f1ba8dd762a4\_b.jpg" data-size="normal" data-rawwidth="1000" data-rawheight="588" class="origin\_image zh-lightbox-thumb" width="1000" data-original="https://pic1.zhimg.com/v2-4dc71fe78c4752645de1f1ba8dd762a4_r.jpg"></p>
<p>Transformer Decoder</p>
<ul>
<li><strong>sub-layer-1</strong>：<strong>Masked multi-head self-attention mechanism</strong>，用来进行self-attention，与Encoder不同：由于是序列生成过程，所以在时刻 i 的时候，大于 i 的时刻都没有结果，只有小于 i 的时刻有结果，因此需要做<strong>Mask</strong>。</li>
<li><strong>sub-layer-2</strong>：<strong>Position-wise Feed-forward Networks</strong>，同Encoder。</li>
<li><strong>sub-layer-3</strong>：<strong>Encoder-Decoder attention计算</strong>。</li>
</ul>
<p><strong>2、Transformer Encoder 与 Transformer Decoder 有哪些不同？</strong></p>
<p>（1）multi-head self-attention mechanism不同，Encoder中不需要使用Masked，而Decoder中需要使用Masked；</p>
<p>（2）Decoder中多了一层Encoder-Decoder attention，这与 self-attention mechanism不同。</p>
<p><strong>3、Encoder-Decoder attention 与self-attention mechanism有哪些不同？</strong></p>
<p>它们都是用了 multi-head计算，不过Encoder-Decoder attention采用传统的attention机制，其中的Query是self-attention mechanism已经计算出的上一时间i处的编码值，Key和Value都是Encoder的输出，这与self-attention mechanism不同。代码中具体体现：</p>
<pre><code> ## Multihead Attention ( self-attention)
            self.dec = multihead_attention(queries=self.dec,
                                           keys=self.dec,
                                           num_units=hp.hidden_units,
                                           num_heads=hp.num_heads,
                                           dropout_rate=hp.dropout_rate,
                                           is_training=is_training,
                                           causality=True,
                                           scope=&quot;self_attention&quot;)

## Multihead Attention ( Encoder-Decoder attention)
            self.dec = multihead_attention(queries=self.dec,
                                           keys=self.enc,
                                           num_units=hp.hidden_units,
                                           num_heads=hp.num_heads,
                                           dropout_rate=hp.dropout_rate,
                                           is_training=is_training,
                                           causality=False,
                                           scope=&quot;vanilla_attention&quot;)
</code></pre><p><strong>4、multi-head self-attention mechanism具体的计算过程是怎样的？</strong></p>
<p><img src="https://pic3.zhimg.com/v2-392692c19c57f5bfa116f7b505dfde7a\_b.jpg" data-size="normal" data-rawwidth="1215" data-rawheight="586" class="origin\_image zh-lightbox-thumb" width="1215" data-original="https://pic3.zhimg.com/v2-392692c19c57f5bfa116f7b505dfde7a_r.jpg"></p>
<p>multi-head self-attention mechanism计算过程</p>
<p>Transformer中的Attention机制由<strong>Scaled Dot-Product Attention</strong>和<strong>Multi-Head Attention</strong>组成，上图给出了整体流程。下面具体介绍各个环节：</p>
<ul>
<li><strong>Expand</strong>：实际上是经过线性变换，生成Q、K、V三个向量；</li>
<li><strong>Split heads</strong>: 进行分头操作，在原文中将原来每个位置512维度分成8个head，每个head维度变为64；</li>
<li><strong>Self Attention</strong>：对每个head进行Self Attention，具体过程和第一部分介绍的一致；</li>
<li><strong>Concat heads</strong>：对进行完Self Attention每个head进行拼接；</li>
</ul>
<p>上述过程公式为：</p>
<p><img src="https://pic3.zhimg.com/v2-c7100e268bcefaa7ff0a1344acc15e7e\_b.jpg" data-caption="" data-size="normal" data-rawwidth="459" data-rawheight="87" class="origin\_image zh-lightbox-thumb" width="459" data-original="https://pic3.zhimg.com/v2-c7100e268bcefaa7ff0a1344acc15e7e_r.jpg"></p>
<p><strong>5、Transformer在GPT和Bert等词向量预训练模型中具体是怎么应用的？有什么变化？</strong></p>
<ul>
<li>GPT中训练的是单向语言模型，其实就是直接应用<strong>Transformer Decoder</strong>；</li>
<li>Bert中训练的是双向语言模型，应用了<strong>Transformer Encoder</strong>部分，不过在Encoder基础上还做了<strong>Masked操作</strong>；</li>
</ul>
<p>BERT Transformer 使用双向self-attention，而GPT Transformer 使用受限制的self-attention，其中每个token只能处理其左侧的上下文。双向 Transformer 通常被称为“Transformer encoder”，而左侧上下文被称为“Transformer decoder”，decoder是不能获要预测的信息的。</p>
<p><strong>Reference</strong></p>
<ol>
<li><a href="https://link.zhihu.com/?target=https%3A//nndl.github.io/" target="_blank" rel="noopener">《神经网络与深度学习》</a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a></li>
<li><a href="https://link.zhihu.com/?target=http%3A//www.chinahadoop.cn/course/1253" target="_blank" rel="noopener">谷歌BERT解析—-2小时上手最强NLP训练模型</a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/RLxWevVWHXgX-UcoxDS70w" target="_blank" rel="noopener">细讲 | Attention Is All You Need</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/37601161" target="_blank" rel="noopener">深度学习中的注意力模型（2017版）</a></li>
</ol>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/13/第一次有人把Apache Flink说的这么明白！/" rel="next" title="第一次有人把Apache Flink说的这么明白！">
                <i class="fa fa-chevron-left"></i> 第一次有人把Apache Flink说的这么明白！
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/headpic.png" alt="Yimitri">
            
              <p class="site-author-name" itemprop="name">Yimitri</p>
              <p class="site-description motion-element" itemprop="description">深度学习 机器学习 网络安全 神经网络</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">14</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">分类</span>
                
              </div>
            

            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/newdimitri" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:Yimitrii@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://ws3.sinaimg.cn/large/005BYqpgly1g1jru8map2j30kw0r241d.jpg" target="_blank" title="Wechat">
                      
                        <i class="fa fa-fw fa-weixin"></i>Wechat</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.zhihu.com/org/peddlepaddleqian-yan-gen-zong/activities" target="_blank" title="知乎">
                      
                        <i class="fa fa-fw fa-mortar-board (alias)"></i>知乎</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#一、Attention机制剖析"><span class="nav-number">1.</span> <span class="nav-text">一、Attention机制剖析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二、Transformer（Attention-Is-All-You-Need）详解"><span class="nav-number">2.</span> <span class="nav-text">二、Transformer（Attention Is All You Need）详解</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yimitri</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>



<!-- 新增访客统计代码 -->

<div class="busuanzi-count">
    <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="site-uv">
      <i class="fa fa-user"></i>
      访问用户： <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 人
    </span>
    <div class="powered-by"></div>
    <span class="site-uv">
      <i class="fa fa-eye"></i>
      访问次数： <span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 次
    </span>
    <!-- 博客字数统计 -->
    <span class="site-pv">
      <i class="fa fa-pencil"></i>
      博客全站共： <span class="post-count"></span> 字
    </span>
</div>
<!-- 新增访客统计代码 END-->
        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  

  

  

</body>
</html>
