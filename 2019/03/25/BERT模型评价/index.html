<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="深度神经网络的超强有效性一直让人疑惑。 经典论文《可视化与理解CNN》（Visualizing and Understanding Convolutional Networks）解释了在图像领域中CNN从低层到高层不断学习出图像的边缘、转角、组合、局部、整体信息的过程，一定层面论证了深度学习的有效性。另一方面，传统的NLP神经网络却并不是那么深，而bert的出现直接将NLP的神经网络加到12层以上">
<meta property="og:type" content="article">
<meta property="og:title" content="BERT模型评价">
<meta property="og:url" content="http://yoursite.com/2019/03/25/BERT模型评价/index.html">
<meta property="og:site_name" content="国家二级薯条试吃员">
<meta property="og:description" content="深度神经网络的超强有效性一直让人疑惑。 经典论文《可视化与理解CNN》（Visualizing and Understanding Convolutional Networks）解释了在图像领域中CNN从低层到高层不断学习出图像的边缘、转角、组合、局部、整体信息的过程，一定层面论证了深度学习的有效性。另一方面，传统的NLP神经网络却并不是那么深，而bert的出现直接将NLP的神经网络加到12层以上">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://5b0988e595225.cdn.sohucs.com/images/20190122/8eb08baa1c7b4112a700d265fbc01077.gif">
<meta property="og:image" content="http://5b0988e595225.cdn.sohucs.com/images/20190122/b97689e12302485399ccec2ff92e8ea7.jpeg">
<meta property="og:image" content="http://5b0988e595225.cdn.sohucs.com/images/20190122/6213bbece4b74a6d8109a5e498563501.jpeg">
<meta property="og:image" content="http://5b0988e595225.cdn.sohucs.com/images/20190122/b17d01f708fb44568320bd40a9a2c89e.jpeg">
<meta property="og:image" content="http://5b0988e595225.cdn.sohucs.com/images/20190122/b8b0da2fc8464ea3aa79643268c35118.jpeg">
<meta property="og:image" content="http://5b0988e595225.cdn.sohucs.com/images/20190122/e134c4c955244dbb859832b120f58cb0.jpeg">
<meta property="og:image" content="http://5b0988e595225.cdn.sohucs.com/images/20190122/e3be9cc1386a4a9093875e5468da7d34.jpeg">
<meta property="og:image" content="http://5b0988e595225.cdn.sohucs.com/images/20190122/a782c12903864b70b1f0fdf695f3ccff.jpeg">
<meta property="og:image" content="http://5b0988e595225.cdn.sohucs.com/images/20190122/f1e5f9129400431ab2c5611806973c44.jpeg">
<meta property="og:image" content="http://5b0988e595225.cdn.sohucs.com/images/20190122/144bdc529a69408f8410c2b0f342827b.jpeg">
<meta property="og:image" content="http://5b0988e595225.cdn.sohucs.com/images/20190122/3464220700594859a59b1703569d75fe.jpeg">
<meta property="og:image" content="http://5b0988e595225.cdn.sohucs.com/images/20190122/21ddc9e905074e11880b502e1e000396.jpeg">
<meta property="og:image" content="http://5b0988e595225.cdn.sohucs.com/images/20190122/59308df6eaae4915a0266c9888d65daf.png">
<meta property="og:image" content="http://5b0988e595225.cdn.sohucs.com/images/20190122/83e834b2bcc141a8a61948b9d1b9f59f.png">
<meta property="og:image" content="http://5b0988e595225.cdn.sohucs.com/images/20190122/52ebe4492b1546cd83b050b297ecc641.jpeg">
<meta property="og:image" content="http://5b0988e595225.cdn.sohucs.com/images/20190122/4fdb08e3bb7f4cc18d0c4a7854b34f71.jpeg">
<meta property="og:image" content="http://5b0988e595225.cdn.sohucs.com/images/20190122/7b0a19dfceb940ea97f3ab496cf63fbb.jpeg">
<meta property="og:image" content="http://5b0988e595225.cdn.sohucs.com/images/20190122/a72a618a4a554d3993e5516138f57d87.jpeg">
<meta property="og:image" content="http://5b0988e595225.cdn.sohucs.com/images/20190122/ad6586cdcf764e4f84c5b777966b6c52.jpeg">
<meta property="og:image" content="http://5b0988e595225.cdn.sohucs.com/images/20190122/f27a4dbc08a24e65b8d264950aba8447.jpeg">
<meta property="og:image" content="http://5b0988e595225.cdn.sohucs.com/images/20190122/57fc7a8fae90406f8d4d23e3ec5adbc7.png">
<meta property="og:updated_time" content="2019-03-29T10:00:16.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="BERT模型评价">
<meta name="twitter:description" content="深度神经网络的超强有效性一直让人疑惑。 经典论文《可视化与理解CNN》（Visualizing and Understanding Convolutional Networks）解释了在图像领域中CNN从低层到高层不断学习出图像的边缘、转角、组合、局部、整体信息的过程，一定层面论证了深度学习的有效性。另一方面，传统的NLP神经网络却并不是那么深，而bert的出现直接将NLP的神经网络加到12层以上">
<meta name="twitter:image" content="http://5b0988e595225.cdn.sohucs.com/images/20190122/8eb08baa1c7b4112a700d265fbc01077.gif">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/03/25/BERT模型评价/">





  <title>BERT模型评价 | 国家二级薯条试吃员</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?179528a02a5e4886fc76ce4097e5c486";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">国家二级薯条试吃员</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">没有我不敢收的红包</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      

      
    </ul>
  

  
</nav>
<script>
    
    window.onload = function(){
        var path = 'https://malizhi.cn'; //这里要改成你博客的地址
        var localhostItem = String(window.location).split(path)[1];
        var LiNode = document.querySelectorAll('#menu > li > a')
        
        for(var i = 0; i< LiNode.length;i++){
            var item = String(LiNode[i].href).split(path)[1];
            if(item == localhostItem && item != undefined){
                LiNode[i].setAttribute('style','border-bottom:1px solid black');
            }
        }
    };

</script>


 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/25/BERT模型评价/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yimitri">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/headpic.png">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="国家二级薯条试吃员">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">BERT模型评价</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-25T22:15:08+08:00">
                2019-03-25
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/03/25/BERT模型评价/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/03/25/BERT模型评价/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  5.1k字
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  17分钟
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><strong>深度神经网络的超强有效性一直让人疑惑。</strong></p>
<p>经典论文《可视化与理解CNN》（Visualizing and Understanding Convolutional Networks）解释了在图像领域中CNN从低层到高层不断学习出图像的边缘、转角、组合、局部、整体信息的过程，一定层面论证了深度学习的有效性。<strong>另一方面，传统的NLP神经网络却并不是那么深，而bert的出现直接将NLP的神经网络加到12层以上。</strong></p>
<p><strong>那么如何理解各层学到了怎样的信息？</strong><br><a id="more"></a>  </p>
<p>本文作者Jesse Vig通过可视化工具对此进行了非常有意义的探究。文章分两部分，第一部分介绍bert中的6种模式，第二部分介绍其底层细节。</p>
<p>可视化BERT之一</p>
<p>在BERT错综复杂的注意力网络中，出现了一些直观的模式。</p>
<p>2018年是自然语言处理领域的转折之年，一系列深度学习模型在智能问答、情感分类等多种NLP 任务上取得了最佳结果。特别是最近谷歌的BERT ，成为了一种“以一当十的模型”，在各种任务上都取得了的极佳的表现。</p>
<p>BERT主要建立在两个核心思想上，这两个思想都包含了NLP 最新进展：（1）Transformer 的架构（2）无监督学习预训练。</p>
<p>Transformer 是一种序列模型，它舍弃了 RNN 的顺序结构，转而采用了一种完全基于注意力的方法。这在经典论文 《Attention Is All You Need》中有具体介绍。</p>
<p>BERT 同时也要经过预训练。它的权重预先通过两个无监督任务学习到。这两个任务是：遮蔽语言模型（masked language model，MLM）和下句一句预测（next sentence prediction）。</p>
<p>因此，对于每个新任务，BERT 不需要从头开始训练。相反，只要在预训练的权重上进行微调（fine-tuning）就行。有关BERT的更多详细信息，可以参考文章《图解BERT》。</p>
<p><strong>BERT是一只多头怪</strong></p>
<p>Bert不像传统的注意力模型那样只使用一个平坦的注意力机制。相反，BERT 使用了多层次的注意力（12或24层，具体取决于模型），并在每一层中包含多个（12或16）注意力“头”。由于模型权重不在层之间共享，因此一个BERT 模型就能有效地包含多达24 x 16 = 384个不同的注意力机制。</p>
<p><strong>可视化BERT</strong></p>
<p>由于BERT 的复杂性，所以很难直观地了解其内部权重的含义。而且一般来说，深度学习模型也是饱受诟病的黑箱结构。所以大家开发了各种可视化工具来辅助理解。</p>
<p>可我却没有找到一个工具能够解释BERT 的注意力模式，来告诉我们它到底在学什么。幸运的是，Tensor2Tensor有一个很好的工具，可用于可视化Transformer 模型中的注意力模式。因此我修改了一下，直接用在BERT的一个pytorch版本上。修改后的界面如下所示。你可以直接在这个Colab notebook (<a href="https://colab.research.google.com/drive/1vlOJ1lhdujVjfH857hvYKIdKPTD9Kid8）里运行，或在Github" target="_blank" rel="noopener">https://colab.research.google.com/drive/1vlOJ1lhdujVjfH857hvYKIdKPTD9Kid8）里运行，或在Github</a> 上找到源码。(<a href="https://github.com/jessevig/bertviz)。" target="_blank" rel="noopener">https://github.com/jessevig/bertviz)。</a></p>
<p><img src="http://5b0988e595225.cdn.sohucs.com/images/20190122/8eb08baa1c7b4112a700d265fbc01077.gif" alt=""></p>
<p>这个工具将注意力看做不同的连线，它们用来连接被更新的位置（左半边）与被注意的位置（右半边）。（译注：可以想象为神经网络是从右向左正向传播的。）不同的颜色分别代表相应的注意头，而线条颜色的深浅代表被注意的强度。在这个小工具的顶部，用户可以选择观察模型的第几层，以及第几个注意力头（通过单击顶部的色块即可，它们分别代表着12个头）。</p>
<p><strong>BERT 到底学了什么？</strong></p>
<p>我使用该工具探索了预训练 BERT 模型各个层和各个头的注意力模式（用全小写（uncased）版本的BERT-Base 模型）。虽然我尝试了不同的输入句子，但为了方便演示，这里只采用以下例句：</p>
<blockquote>
<p><strong>句子A：I went to the store.</strong></p>
<p><strong>句子B：At the store, I bought fresh strawberries.</strong></p>
</blockquote>
<p>BERT 用 WordPiece工具来进行分词，并插入特殊的分离符（[CLS]，用来分隔样本）和分隔符（[SEP]，用来分隔样本内的不同句子）。</p>
<p>因此实际输入序列为： [CLS] i went to the store . [SEP] at the store , i bought fresh straw ##berries . [SEP]</p>
<p>在探索中，我发现了一些特别显著的令人惊讶的注意力模式。下面是我确认的六种关键模式，将产生每一种模式的特定层和头都进行可视化展示。</p>
<p><strong>模式1：注意下一个词</strong></p>
<p>在这种模式中，每个位置主要注意序列中的下一个词（token）。下面将看到第2层0号头的一个例子。（所选头部由顶部颜色条中突出的显示色块表示。）</p>
<p><img src="http://5b0988e595225.cdn.sohucs.com/images/20190122/b97689e12302485399ccec2ff92e8ea7.jpeg" alt=""></p>
<p>模式1：注意下一个词。</p>
<p>左：所有词的注意力。 右：所选词的注意力权重（“i”）</p>
<p>左边显示了所有词的注意力，而右侧图显示一个特定词（“i”）的注意力。在这个例子中，“i”几乎所有的注意力都集中在“went”上，即序列中的下一个词。</p>
<p>在左侧，可以看到 [SEP]符号不符合这种注意力模式，因为[SEP]的大多数注意力被引导到了[CLS]上，而不是下一个词。因此，这种模式似乎主要在每个句子内部出现。</p>
<p>该模式与后向RNN 有关，其状态的更新是从右向左依次进行。模式1出现在模型的多个层中，在某种意义上模拟了RNN 的循环更新。</p>
<p><strong>模式2：注意前一个词</strong></p>
<p>在这种模式中，大部分注意力都集中在句子的前一个词上。例如，下图中“went”的大部分注意力都指向前一个词“i”。</p>
<p>这个模式不像上一个那样显著。有一些注意力也分散到其他词上了，特别是[SEP]符号。与模式1一样，这与RNN 有些类似，只是这种情况下更像前向RNN。</p>
<p><img src="http://5b0988e595225.cdn.sohucs.com/images/20190122/6213bbece4b74a6d8109a5e498563501.jpeg" alt=""></p>
<p>模式2：注意前一个词。</p>
<p>左：所有词的注意力。 右：所选词的注意力权重（“went”）</p>
<p><strong>模式3：注意相同或相关的单词</strong></p>
<p>这种模式注意相同或相关的单词，包括其本身。在下面的例子中，第一次出现的“store”的大部分注意力都是针对自身和第二次出现的“store”。这种模式并不像其他一些模式那样显著，注意力会分散在许多不同的词上。</p>
<p><img src="http://5b0988e595225.cdn.sohucs.com/images/20190122/b17d01f708fb44568320bd40a9a2c89e.jpeg" alt=""></p>
<p>模式3：注意相同/相关的词。</p>
<p>左：所有词的注意力。 右：所选词的注意权重（“store”）</p>
<p><strong>模式4：注意“其他”句子中相同或相关词</strong></p>
<p>这种模式注意另一个句子中相同或相关的单词。例如，第二句中“store”的大部分注意力都指向第一句中的“store”。可以想象这对于下句预测任务（BERT预训练任务的一部分）特别有用，因为它有助于识别句子之间的关系。</p>
<p><img src="http://5b0988e595225.cdn.sohucs.com/images/20190122/b8b0da2fc8464ea3aa79643268c35118.jpeg" alt=""></p>
<p>模式4：注意其他句子中相同/相关的单词。</p>
<p>左：所有词的注意力。 右：所选词的注意权重（“store”）</p>
<p><strong>模式5：注意能预测该词的其他单词</strong></p>
<p>这种模式似乎是更注意能预测该词的词，而不包括该词本身。在下面的例子中，“straw”的大部分注意力都集中在“##berries”上（strawberries 草莓，因为WordPiece分开了），而“##berries”的大部分注意力也都集中在“straw”上。</p>
<p><img src="http://5b0988e595225.cdn.sohucs.com/images/20190122/e134c4c955244dbb859832b120f58cb0.jpeg" alt=""></p>
<p>模式5：注意能预测该单词的其他单词。</p>
<p>左：所有词的注意力。 右：所选词的注意力（“## berries”）</p>
<p>这个模式并不像其他模式那样显著。例如，词语的大部分注意力都集中在定界符（[CLS]）上，而这是下面讨论的模式6的特征。</p>
<p><strong>模式6：注意分隔符</strong></p>
<p>在这种模式中，词语的大部分注意力都集中在分隔符[CLS]或 [SEP]上。在下面的示例中，大部分注意力都集中在两个 [SEP]符号上。这可能是模型将句子级状态传播到单个词语上的一种方式。</p>
<p><img src="http://5b0988e595225.cdn.sohucs.com/images/20190122/e3be9cc1386a4a9093875e5468da7d34.jpeg" alt=""></p>
<p>模式6：注意分隔符。 左：所有词的注意力。 右：所选词的注意权重（“store”）</p>
<p><strong>说明</strong></p>
<p>其实数据可视化有点像罗夏墨迹测验（译注：这种测验叫人解释墨水点绘的图形以判断其性格）：我们的解释可能会被我们的主观信念和期望所影响。虽然上面的一些模式非常显著，但其他模式却有点主观，所以这些解释只能作为初步观察。</p>
<p>此外，上述6种模式只是描述了BERT 的粗略注意力结构，并没有试图去描述注意力可能捕获到的语言学（linguistic）层面的模式。例如，在模式3和4中，其实可以表现为许多其他不同类型的“相关性”，例如同义关系、共同指代关系等。</p>
<p>而且，如果能看到注意力头是否抓取到不同类型的语义和句法关系，那将会非常有趣。</p>
<p>可视化BERT之二：探索注意力机制的内部细节一</p>
<p>在这里，一个新的可视化工具将展示BERT如何形成其独特的注意力模式。</p>
<p><img src="http://5b0988e595225.cdn.sohucs.com/images/20190122/a782c12903864b70b1f0fdf695f3ccff.jpeg" alt=""></p>
<p>在上文中，我讲解了BERT的注意力机制是如何呈现出多种模式的。例如，一个注意力头会主要注意序列中的下一个词；而另一个注意力头会主要注意序列中的前一个词（具体看下方图示）。在这两种情况中，BERT在本质上都是学习一种类似RNN的序列更新的模式。之后，我们也将展示BERT是如何建模词袋模型（Bag-of-Words）的。</p>
<p><img src="http://5b0988e595225.cdn.sohucs.com/images/20190122/f1e5f9129400431ab2c5611806973c44.jpeg" alt=""></p>
<p>通过BERT学习下一个单词和上一个单词的注意力模式</p>
<p>那么BERT是如何学到这些极好的特性呢？为了解决这个问题，我从第一部分扩展了可视化工具来更深入地探索BERT——揭示提供BERT强大建模能力的神经元。你可以在这个Colab notebook或者 Github上找到这个可视化工具。</p>
<p>最初的可视化工具（基于由Llion Jones出色完成的Tensor2Tensor）尝试来解释什么是注意力：也就是说，BERT到底在学习什么样的注意力结构？那么它是怎样学到的呢？为了解决这个问题，我添加了一个注意力细节视图，来可视化注意力的计算过程。详细视图通过点击⊕图标按钮来查看。你可以看到以下的一个demo示例，或直接跳到屏幕截图。</p>
<p><strong>可视化工具概览</strong></p>
<p>BERT有点像鲁布·戈德堡机（译注：是一种被设计得过度复杂的机械组合，以迂回曲折的方法去完成一些其实是非常简单的工作，例如倒一杯茶，或打一只蛋。），尽管每个组件都非常直观，但是系统整体很难把握。现在我将通过可视化工具介绍BERT注意力架构的各个部分。（想了解有关BERT的全部教程，推荐《图解transformer》 和《图解BERT》这两篇文章。）</p>
<p>下方展示了新的注意力细节视图。图中正值是蓝色的，负值是橙色的，颜色的深浅反映了取值的大小。所有的向量都是64维的，并且作用于某个特定的注意力头上。和最初的可视化工具类似，连接线颜色的深浅代表了单词之间的注意力强度。</p>
<p><img src="http://5b0988e595225.cdn.sohucs.com/images/20190122/144bdc529a69408f8410c2b0f342827b.jpeg" alt=""></p>
<p>让我们结合图分析一下：</p>
<p><strong>查询向量q ：</strong>查询向量q是左边正在进行注意力过程的单词/位置的编码，也就是说由它来“查询”其他的单词/位置。在上述的例子中，“the”（选中的单词）的查询向量标注出来了。</p>
<p><strong>键向量k：</strong>键向量k是右边正在“被注意”的单词的编码。如下所述，键向量和查询向量决定了单词被注意程度的得分。</p>
<p><strong>q×k (element-wise)</strong>:查询向量和键向量的逐元素积（译注：element-wise product， 也叫哈达玛积/Hadamard product）。这个逐元素积是通过选定的查询向量和每个键向量计算得到的。这是点积（逐元素乘积的和）的前导。由于它展示了查询向量中的单个元素和键向量对点积的贡献，因此将其可视化。选定的查询向量和每个键向量的点积。得到的是非归一化的注意力得分。</p>
<p><strong>Softmax</strong>:所有目标单词的q·k / 8的softmax值。这一步实现了注意力得分的归一化，保证了值为正的且和为1。常量8是向量长度（64）的开方。论文（<a href="https://arxiv.org/pdf/1706.03762.pdf）描述了这样做的原因。" target="_blank" rel="noopener">https://arxiv.org/pdf/1706.03762.pdf）描述了这样做的原因。</a></p>
<p><strong>解析BERT的注意力模式</strong></p>
<p>在第一部分文章中，我在BERT的注意力头的结构中发现了一些模式。来看看我们是否能使用新的可视化工具来理解BERT是如何形成这些模式的。</p>
<p><strong>注意分隔符</strong></p>
<p>让我们以一个简单的例子起手，这个例子中大多数注意力是聚焦于分隔符[SEP] 的（第一部分文章中的模式6）。如第一部分文章中所描述的，这个模式可能是BERT用来将句子级的状态传播到单词级状态的一种方式。</p>
<p><img src="http://5b0988e595225.cdn.sohucs.com/images/20190122/3464220700594859a59b1703569d75fe.jpeg" alt=""></p>
<p>基于BERT预训练模型的第7层3号头，聚焦分隔符注意力模式。</p>
<p>所以，BERT是如何直接聚焦于[SEP]符号的呢？来看看可视化工具。下面是上述例子的注意力细节视图。</p>
<p><img src="http://5b0988e595225.cdn.sohucs.com/images/20190122/21ddc9e905074e11880b502e1e000396.jpeg" alt=""></p>
<p>在键向量列，两个出现[SEP]处的键向量有显著的特点：它们都有少量的高正值（蓝色）和低负值（橘色）的激活神经元，以及非常多的接近0的（浅蓝，浅橘或白色）的神经元。</p>
<p><img src="http://5b0988e595225.cdn.sohucs.com/images/20190122/59308df6eaae4915a0266c9888d65daf.png" alt=""></p>
<p>第一个分隔符[SEP]的键向量。</p>
<p>查询向量q会通过那些激活神经元来匹配[SEP]键向量，会使元素內积q×k产生较高的值，如下例子所示：</p>
<p><img src="http://5b0988e595225.cdn.sohucs.com/images/20190122/83e834b2bcc141a8a61948b9d1b9f59f.png" alt=""></p>
<p>第一个“the”的查询向量；第一个[SEP]的键向量；两个向量的逐元素积。</p>
<p>其他单词的查询向量也遵循相似的模式；它们通过同一组神经元来匹配[SEP]键向量。因此，BERT似乎指定了一小部分神经元作为“[SEP]-匹配神经元”，而查询向量也通过这些相同位置的值来匹配[SEP]键向量。这就是注意分隔符[SEP]的注意力模式。</p>
<p><strong>注意句子：词袋模型（Bag of Words）</strong></p>
<p>这是一个不太常见的模式，在第一部分文章中没有具体讨论。在这种模式中，注意力被平均的分配到句子中的每个单词上。</p>
<p><img src="http://5b0988e595225.cdn.sohucs.com/images/20190122/52ebe4492b1546cd83b050b297ecc641.jpeg" alt=""></p>
<p>基于BERT预训练模型的第0层0号头，专注句子的注意力模式</p>
<p>这个模式的作用是将句子级的状态分配到单词级上。BERT在这里本质上是通过对所有词嵌入进行几乎相等权重的加权平均操作来计算一个词袋模型。词嵌入就是我们之前提到的值向量。</p>
<p>那么BERT是怎样处理查询向量和键向量来形成这种注意力模式的呢？让我们再来看看注意力细节视图；</p>
<p><img src="http://5b0988e595225.cdn.sohucs.com/images/20190122/4fdb08e3bb7f4cc18d0c4a7854b34f71.jpeg" alt=""></p>
<p>基于BERT预训练模型的第0层0号头，专注句子的注意力模式细节视图。</p>
<p>在q×k这列，我们能看到一个清晰的模式：少量神经元（2-4个）控制着注意力得分的计算。当查询向量和键向量在同个句子中时（上例中第一个句子），这些神经元的乘积显示出较高的值（蓝色）。当查询向量和键向量在不同句子中时，在这些相同的位置上，乘积是负的（橘色），如下例子所示：</p>
<p><img src="http://5b0988e595225.cdn.sohucs.com/images/20190122/7b0a19dfceb940ea97f3ab496cf63fbb.jpeg" alt=""></p>
<p>在同个句子中（左边）q*k的逐元素积很高，在不同句子中（右边）q*k的逐元素积很低。</p>
<p>当查询向量和键向量都来自第一个句子中时，它们在激活神经元上往往有相同的符号，因此会产生一个正积。当查询向量来自第一个句子中时，键向量来自第二个句子时，相同地方的神经元会有相反的符号，因此会产生一个负值。</p>
<p>但是BERT是怎么知道“句子”这个概念的？尤其是在神经网络第一层中，更高的抽象信息还没有形成的时候。这个答案就是添加到输入层（见下图）的句子级嵌入（sentence-level embeddings）。这些句子嵌入的编码信息传递到下层的变量中，即查询向量和键向量，并且使它们能够获取到特定句子的值。</p>
<p><img src="http://5b0988e595225.cdn.sohucs.com/images/20190122/a72a618a4a554d3993e5516138f57d87.jpeg" alt=""></p>
<p>句子A和B的部分嵌入以及位置嵌入被添加到词嵌入中</p>
<p>（来自BERT论文（<a href="https://arxiv.org/pdf/1810.04805.pdf））" target="_blank" rel="noopener">https://arxiv.org/pdf/1810.04805.pdf））</a></p>
<p><strong>注意下一个词</strong></p>
<p>在这种注意力模式中，除了分隔符，其他所有的注意力都集中在输入序列的下个单词上。</p>
<p><img src="http://5b0988e595225.cdn.sohucs.com/images/20190122/ad6586cdcf764e4f84c5b777966b6c52.jpeg" alt=""></p>
<p>基于BERT预训练模型的第2层0号头，注意下一个词的注意力模式。</p>
<p>这个模式能够使BERT捕获序列关系，如二元语法（bigrams）。我们来查看它的注意力细节视图；</p>
<p><img src="http://5b0988e595225.cdn.sohucs.com/images/20190122/f27a4dbc08a24e65b8d264950aba8447.jpeg" alt=""></p>
<p>我们看到查询向量“the”和键向量“store”（下个单词）的乘积在大多数神经元中是很高的正值。对于下一个单词之外的其他单词，q*k乘积包含着一些正值和负值。最终的结果是“the”和“store”之间的注意力得分很高。</p>
<p>对于这种注意力模式，大量的神经元参与到注意力得分中。而且这些神经元根据词位置的不同而不同，如下所示：</p>
<p><img src="http://5b0988e595225.cdn.sohucs.com/images/20190122/57fc7a8fae90406f8d4d23e3ec5adbc7.png" alt=""></p>
<p>当i取2,4,8的时候，在位置i处的查询向量和在j = i+1处的键向量的逐元素积。注意激活神经元在每一个例子中都不同。</p>
<p>这种方式不同于注意分隔符以及注意句子的注意力模式，它们是由少量固定的神经元来决定注意力得分的。对于这两种模式，只有少量的神经元是必须的，因此这两种模式都很简单，并且在被注意的单词上都没多少变化。与它们相反，注意下个单词的注意力模式需要追踪512个单词（译注：在BERT中每个样本最多512个单词。）中到底是哪个是被一个给定的位置注意的，即哪个是下一个单词。为了实现这个功能，需要产生一系列查询向量和键向量，其中每个查询向量会有从512个键向量有唯一一个匹配。因此使用少量神经元很难完成这个任务。</p>
<p>那么BERT是如何能够生成这些查询向量和键向量呢？答案就在BERT的位置嵌入（position embeddings），它在输入层（见图1）中被添加到词嵌入（word embeddings）中。BERT在输入序列中学习512个独特的位置嵌入，这些指定位置的信息能通过模型流入到键向量和查询向量中。</p>
<p><strong>大家一起来探索</strong></p>
<p>欢迎大家一起来试试这个可视化工具，并分享自己的发现。</p>
<p>Colab: <a href="https://colab.research.google.com/drive/1Nlhh2vwlQdKleNMqpmLDBsAwrv_7NnrB" target="_blank" rel="noopener">https://colab.research.google.com/drive/1Nlhh2vwlQdKleNMqpmLDBsAwrv_7NnrB</a></p>
<p>Github:</p>
<p><a href="https://github.com/jessevig/bertviz" target="_blank" rel="noopener">https://github.com/jessevig/bertviz</a></p>
<p>PS:非常感谢Llion Jones开发的原始Tensor2Tensor可视化工具。</p>
<p>原文链接：</p>
<p><a href="https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77" target="_blank" rel="noopener">https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77</a></p>
<p><a href="https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1?gi=88e6da975567" target="_blank" rel="noopener">https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1?gi=88e6da975567</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/03/24/BERT算法原理解析/" rel="next" title="BERT算法原理解析">
                <i class="fa fa-chevron-left"></i> BERT算法原理解析
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/03/26/图解BERT/" rel="prev" title="图解BERT">
                图解BERT <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/headpic.png" alt="Yimitri">
            
              <p class="site-author-name" itemprop="name">Yimitri</p>
              <p class="site-description motion-element" itemprop="description">深度学习 网络安全</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">19</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/newdimitri" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:Yimitrii@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://ws3.sinaimg.cn/large/005BYqpgly1g1jru8map2j30kw0r241d.jpg" target="_blank" title="Wechat">
                      
                        <i class="fa fa-fw fa-weixin"></i>Wechat</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.zhihu.com/org/peddlepaddleqian-yan-gen-zong/activities" target="_blank" title="知乎">
                      
                        <i class="fa fa-fw fa-mortar-board (alias)"></i>知乎</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yimitri</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">59.1k</span>
  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>



<!-- 新增访客统计代码 -->

<div class="busuanzi-count">
    <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="site-uv">
      <i class="fa fa-user"></i>
      访问用户： <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 人
    </span>
    <div class="powered-by"></div>
    <span class="site-uv">
      <i class="fa fa-eye"></i>
      访问次数： <span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 次
    </span>
    <!-- 博客字数统计 -->
    <span class="site-pv">
      <i class="fa fa-pencil"></i>
      博客全站共： <span class="post-count">59.1k</span> 字
    </span>
</div>
<!-- 新增访客统计代码 END-->
        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://于祥.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2019/03/25/BERT模型评价/';
          this.page.identifier = '2019/03/25/BERT模型评价/';
          this.page.title = 'BERT模型评价';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://于祥.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  

  

  

</body>
</html>
