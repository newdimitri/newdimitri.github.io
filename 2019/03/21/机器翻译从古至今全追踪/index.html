<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="何中军，百度机器翻译技术负责人。本文根据作者2018年12月在全球架构师峰会上的特邀报告整理而成。  本文章分为以下5个部分：  机器翻译基本原理，介绍机器翻译原理、主要挑战、发展历程，及评价方法 神经网络机器翻译，介绍近年来迅速崛起的神经网络机器翻译 技术挑战，尽管神经网络机器翻译取得一系列较大的进展，但是仍然面临诸多挑战； 典型应用，机器翻译在生产、生活、学习等方面起到越来越大的作用 未来发">
<meta property="og:type" content="article">
<meta property="og:title" content="机器翻译从古至今全追踪">
<meta property="og:url" content="http://yoursite.com/2019/03/21/机器翻译从古至今全追踪/index.html">
<meta property="og:site_name" content="国家二级薯条试吃员">
<meta property="og:description" content="何中军，百度机器翻译技术负责人。本文根据作者2018年12月在全球架构师峰会上的特邀报告整理而成。  本文章分为以下5个部分：  机器翻译基本原理，介绍机器翻译原理、主要挑战、发展历程，及评价方法 神经网络机器翻译，介绍近年来迅速崛起的神经网络机器翻译 技术挑战，尽管神经网络机器翻译取得一系列较大的进展，但是仍然面临诸多挑战； 典型应用，机器翻译在生产、生活、学习等方面起到越来越大的作用 未来发">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://pic3.zhimg.com/v2-8dd703919e27676c688a678bc43401de\_b.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-8dd703919e27676c688a678bc43401de_hd.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/v2-f6b2167ffa61575ac6139d42cf470cc3\_b.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/v2-93b69dff2269b139c59ddca86d5709a6\_b.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/v2-bf27649c4cb6a29148bae433a4493574\_b.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/v2-eaf3cdac3ed00d888d0bfa9880e6c503\_b.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/v2-40077f90ceb8b5fd04d895f2f0de77ac\_b.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/v2-361a115f73f2169f4d45044bfdd3c3e5\_b.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/v2-5a0b2ab77f039e8c22f3838d0105f773\_b.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/v2-de112febc7d5802b87936ba60d8e4679\_b.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/v2-93c390d5cf3aca668b8638b358c264c2\_b.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/v2-01cce5b9aaa3a77641419c7f591f7d0d\_b.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/v2-2a2fa759ea3a8604a0e1e87341242dd6\_b.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/v2-0ff6dbb64fcdb08d56a7d117b3954973\_b.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/v2-98d6a9664e8f0b9920c8ce497493ea94\_b.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/v2-c5585f838173b32985730d327996bc64\_b.gif">
<meta property="og:image" content="https://pic1.zhimg.com/v2-c5585f838173b32985730d327996bc64_b.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/v2-578eff847c1f7303a590d0db4caa4b36\_b.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/v2-abc1cddace800cfdaa683c3c817b2a85\_b.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/v2-f7dd5a4d85cde44515a4a989bb6d83d8\_b.gif">
<meta property="og:image" content="https://pic1.zhimg.com/v2-f7dd5a4d85cde44515a4a989bb6d83d8_b.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/v2-0425bdc29df51560c25ddebf7817b9fb\_b.gif">
<meta property="og:image" content="https://pic4.zhimg.com/v2-0425bdc29df51560c25ddebf7817b9fb_b.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/v2-0ee541bfeebc736e19d8f593fcda8096\_b.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/v2-27d356b60af479a2f16e154b55fbe10d\_b.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/v2-740b6b55a3cf9e88ec7fdc46b9b13496\_b.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/v2-923fdb64b8fc8fe234c9b33e623ad640\_b.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/v2-c2d3ac6fc1bdea882884f5821df509e7\_b.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/v2-355f982a017a08413d302c45e52adab4\_b.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/v2-6fa9a34d85ef8eec47a48cae5a916634\_b.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/v2-4b61c8fc4267a708a971154c25af8470\_b.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/v2-72cdceae6218e5195421de479fefab76\_b.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/v2-5988edf384c0e9455e30fc736a700301\_b.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/v2-f5b0e532b1ba58cf937aa4bf88148771\_b.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/v2-59708dbcab70ace657847a68eb5b0304\_b.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/v2-0f33018f35c75e62d0e3b62598c11c22\_b.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/v2-f915521420f0ad589946eba80cacf5e6\_b.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/v2-e9eb09ee335deb9284ff6b3a2a25c445\_b.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/v2-5ddd5545986d67d895663f97c9ff2f83\_b.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/v2-dd24d302ec255dc851f30f5742eb4f95\_b.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/v2-af4122d543eaaaa1372d3cdd74c068a5\_b.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/v2-3f6983204f4c356ca24246244269ea80\_b.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/v2-60a3e9d76358b3d92011314792f433cf\_b.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/v2-56538dd438aa6b8b92052c8c5e2aec52\_b.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/v2-69e70cd0e85369c34e67d6cf84d54f5d\_b.gif">
<meta property="og:image" content="https://pic2.zhimg.com/v2-69e70cd0e85369c34e67d6cf84d54f5d_b.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/v2-45b92134413ec07fb3219db941b1ddf7\_b.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/v2-57f99be6ec49fa41d6cf77036038b85b\_b.gif">
<meta property="og:image" content="https://pic4.zhimg.com/v2-57f99be6ec49fa41d6cf77036038b85b_b.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/v2-7f7b4fdeeec5ce68652111d19ec5cf8a\_b.gif">
<meta property="og:image" content="https://pic3.zhimg.com/v2-7f7b4fdeeec5ce68652111d19ec5cf8a_b.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/v2-14e1045e08318a8d8f6a3755be4e5060\_b.jpg">
<meta property="og:updated_time" content="2019-03-29T09:45:53.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器翻译从古至今全追踪">
<meta name="twitter:description" content="何中军，百度机器翻译技术负责人。本文根据作者2018年12月在全球架构师峰会上的特邀报告整理而成。  本文章分为以下5个部分：  机器翻译基本原理，介绍机器翻译原理、主要挑战、发展历程，及评价方法 神经网络机器翻译，介绍近年来迅速崛起的神经网络机器翻译 技术挑战，尽管神经网络机器翻译取得一系列较大的进展，但是仍然面临诸多挑战； 典型应用，机器翻译在生产、生活、学习等方面起到越来越大的作用 未来发">
<meta name="twitter:image" content="https://pic3.zhimg.com/v2-8dd703919e27676c688a678bc43401de\_b.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/03/21/机器翻译从古至今全追踪/">





  <title>机器翻译从古至今全追踪 | 国家二级薯条试吃员</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?179528a02a5e4886fc76ce4097e5c486";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">国家二级薯条试吃员</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">没有我不敢收的红包</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      

      
    </ul>
  

  
</nav>
<script>
    
    window.onload = function(){
        var path = 'https://malizhi.cn'; //这里要改成你博客的地址
        var localhostItem = String(window.location).split(path)[1];
        var LiNode = document.querySelectorAll('#menu > li > a')
        
        for(var i = 0; i< LiNode.length;i++){
            var item = String(LiNode[i].href).split(path)[1];
            if(item == localhostItem && item != undefined){
                LiNode[i].setAttribute('style','border-bottom:1px solid black');
            }
        }
    };

</script>


 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/21/机器翻译从古至今全追踪/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yimitri">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/headpic.png">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="国家二级薯条试吃员">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">机器翻译从古至今全追踪</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-21T22:15:08+08:00">
                2019-03-21
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/03/21/机器翻译从古至今全追踪/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/03/21/机器翻译从古至今全追踪/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  9.1k字
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  30分钟
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <blockquote>
<p>何中军，百度机器翻译技术负责人。本文根据作者2018年12月在全球架构师峰会上的特邀报告整理而成。</p>
</blockquote>
<p><strong>本文章分为以下5个部分：</strong></p>
<ul>
<li>机器翻译基本原理，介绍机器翻译原理、主要挑战、发展历程，及评价方法</li>
<li>神经网络机器翻译，介绍近年来迅速崛起的神经网络机器翻译</li>
<li>技术挑战，尽管神经网络机器翻译取得一系列较大的进展，但是仍然面临诸多挑战；</li>
<li>典型应用，机器翻译在生产、生活、学习等方面起到越来越大的作用</li>
<li>未来发展，展望未来发展趋势</li>
</ul>
<a id="more"></a>  
<p><strong>机器翻译基本原理</strong></p>
<p><img src="https://pic3.zhimg.com/v2-8dd703919e27676c688a678bc43401de\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="601" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic3.zhimg.com/v2-8dd703919e27676c688a678bc43401de_r.jpg"></p>
<p><img src="https://pic3.zhimg.com/80/v2-8dd703919e27676c688a678bc43401de_hd.jpg" alt=""></p>
<p>简单来说，机器翻译就是把一种语言翻译成另外一种语言，在这里，我用的例子都是从中文翻译成英文。上面的句子用Source标记，即源语言，下面用Target标记，即目标语言，机器翻译任务就是把源语言的句子翻译成目标语言的句子。</p>
<p>机器翻译是人工智能的终极目标之一，面临如下国际公认的挑战。</p>
<p><img src="https://pic4.zhimg.com/v2-f6b2167ffa61575ac6139d42cf470cc3\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="608" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic4.zhimg.com/v2-f6b2167ffa61575ac6139d42cf470cc3_r.jpg"></p>
<p>第一个挑战，译文选择。在翻译一个句子的时候，会面临很多选词的问题，因为语言中一词多义的现象比较普遍。比如这个例子中，源语言句子中的『看』，可以翻译成『look』、『watch』 『read 』和 『see』等词，如果不考虑后面的宾语『书』的话，这几个译文都对。在这个句子中，只有机器翻译系统知道『看』的宾语『书』，才能做出正确的译文选择，把『看』翻译为『read』 ，『read a book』。译文选择是机器翻译面临的第一个挑战。</p>
<p><img src="https://pic3.zhimg.com/v2-93b69dff2269b139c59ddca86d5709a6\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="603" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic3.zhimg.com/v2-93b69dff2269b139c59ddca86d5709a6_r.jpg"></p>
<p>第二个挑战，是词语顺序的调整。由于文化及语言发展上的差异，我们在表述的时候，有时候先说这样一个成份，后面说另外一个成份 ，但是，在另外一种语言中，这些语言成分的顺序可能是完全相反的。比如在这个例子中，『在周日』，这样一个时间状语在英语中习惯上放在句子后面。再比如，像中文和日文的翻译，中文的句法是『主谓宾』，而日文的句法是『主宾谓』，日文把动词放在句子最后。比如中文说『我吃饭』，那么日语呢就会说『我饭吃』。当句子变长时，语序调整会更加复杂。</p>
<p><img src="https://pic1.zhimg.com/v2-bf27649c4cb6a29148bae433a4493574\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="599" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic1.zhimg.com/v2-bf27649c4cb6a29148bae433a4493574_r.jpg"></p>
<p>第三个挑战，数据稀疏。据不完全统计，现在人类的语言大约有超过五千种。现在的机器翻译技术大部分都是基于大数据的，只有在大量的数据上训练才能获得一个比较好的效果。而实际上，语言数量的分布非常不均匀的。右边的饼图显示了中文相关语言的一个分布情况，大家可以看到，百分之九十以上的都是中文和英文的双语句对，中文和其他语言的资源呢，是非常少的。在非常少的数据上，想训练一个好的系统是非常困难的。</p>
<p><strong>机器翻译发展历程</strong></p>
<p><img src="https://pic4.zhimg.com/v2-eaf3cdac3ed00d888d0bfa9880e6c503\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="608" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic4.zhimg.com/v2-eaf3cdac3ed00d888d0bfa9880e6c503_r.jpg"></p>
<p>从1949年翻译备忘录提出到现在，大约过了七十多年。这期间，机器翻译经历了多个不同的发展阶段，也涌现出了很多方法。总结起来主要有三类，一开始是基于规则的方法，然后发展为基于统计的方法。一直到最近几年出现的基于神经网络的方法。下面我分别来简单介绍一下这几个方法的原理。</p>
<p><img src="https://pic1.zhimg.com/v2-40077f90ceb8b5fd04d895f2f0de77ac\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="602" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic1.zhimg.com/v2-40077f90ceb8b5fd04d895f2f0de77ac_r.jpg"></p>
<p>基于规则的翻译，翻译知识来自人类专家。找人类语言学家来写规则，这一个词翻译成另外一个词。这个成分翻译成另外一个成分，在句子中的出现在什么位置，都用规则表示出来。这种方法的优点是直接用语言学专家知识，准确率非常高。缺点是什么呢？它的成本很高，比如说要开发中文和英文的翻译系统，需要找同时会中文和英文的语言学家。要开发另外一种语言的翻译系统，就要再找懂另外一种语言的语言学家。因此，基于规则的系统开发周期很长，成本很高。</p>
<p>此外，还面临规则冲突的问题。随着规则数量的增多，规则之间互相制约和影响。有时为了解决一个问题而写的一个规则，可能会引起其他句子的翻译，带来一系列问题。而为了解决这一系列问题，不得不引入更多的规则，形成恶性循环。</p>
<p><img src="https://pic2.zhimg.com/v2-361a115f73f2169f4d45044bfdd3c3e5\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="604" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic2.zhimg.com/v2-361a115f73f2169f4d45044bfdd3c3e5_r.jpg"></p>
<p>大约到了上世纪九十年代出现了基于统计的方法，我们称之为统计机器翻译。统计机器翻译系统对机器翻译进行了一个数学建模。可以在大数据的基础上进行训练。</p>
<p>它的成本是非常低的，因为这个方法是语言无关的。一旦这个模型建立起来以后，对所有的语言都可以适用。统计机器翻译是一种基于语料库的方法，所以如果是在数据量比较少的情况下，就会面临一个数据稀疏的问题。同时，也面临另外一个问题，其翻译知识来自大数据的自动训练，那么如何加入专家知识？ 这也是目前机器翻译方法所面临的一个比较大挑战。  </p>
<p><img src="https://pic4.zhimg.com/v2-5a0b2ab77f039e8c22f3838d0105f773\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="604" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic4.zhimg.com/v2-5a0b2ab77f039e8c22f3838d0105f773_r.jpg"></p>
<p>翻译知识主要来自两类训练数据：平行语料，一句中文一句英文，并且这句中文和英文，是互为对应关系的，也叫双语语料；单语语料，比如说只有英文我们叫单语语料。</p>
<p>从平行语料中能学到什么呢？翻译模型能学到类似于词典这样的一个表，一般称为『短语表』。比如说『在周日』可以翻译成『on Sunday』。后面还有一个概率，衡量两个词或者短语对应的可能性。这样，『短语表』就建立起两种语言之间的一种桥梁关系。</p>
<p>那么我们能够用单语语料来做什么呢？我们用单语语料来训练语言模型。语言模型是做什么事情的呢？就是衡量一个句子在目标语言中是不是地道，是不是流利。比如这里说『read a book』，这个表述是没有问题的，『read a 』后面跟一个『book 』这个词的概率可能是0.5，那么如果说『read a TV』呢？可能性就很低。因为这不符合目标语言的语法。</p>
<p>所以，翻译模型建立起两种语言的桥梁，语言模型是衡量一个句子在目标语言中是不是流利和地道。这两种模型结合起来，加上其他的一些特征，就组成了一个统计机器翻译这样的一个公式。</p>
<p><strong>神经网络机器翻译</strong></p>
<p><img src="https://pic2.zhimg.com/v2-de112febc7d5802b87936ba60d8e4679\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="606" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic2.zhimg.com/v2-de112febc7d5802b87936ba60d8e4679_r.jpg"></p>
<p>神经网络翻译近年来迅速崛起。相比统计机器翻译而言，神经网络翻译从模型上来说相对简单，它主要包含两个部分，一个是编码器，一个是解码器。编码器是把源语言经过一系列的神经网络的变换之后，表示成一个高维的向量。解码器负责把这个高维向量再重新解码（翻译）成目标语言。</p>
<p>随着深度学习技术的发展，大约从2014年神经网络翻译方法开始兴起。2015年百度发布了全球首个互联网神经网络翻译系统。短短3、4年的时间，神经网络翻译系统在大部分的语言上已经超过了基于统计的方法。</p>
<p>目前，评价机器翻译的译文质量主要有两种方式。第一种，人工评价。一说人工评价，大家第一时间就会想到『信、达、雅』，这是当年严复老先生提出来。我们用『信』来衡量忠实度，语言是为了交流的，『信』衡量译文是不是忠实地反映了原文所要表达的意思。『达』可以理解为流利度，就像刚才语言模型那样衡量的，译文是不是在目标语言中是一个流畅、地道的表达。至于『雅』，相对比较难衡量，这是仁者见仁、智者见智的。目前来说，机器翻译水平还远没有达到可以用『雅』来衡量的状态。  </p>
<p><img src="https://pic3.zhimg.com/v2-93c390d5cf3aca668b8638b358c264c2\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="606" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic3.zhimg.com/v2-93c390d5cf3aca668b8638b358c264c2_r.jpg"></p>
<p>第二种，自动评价。自动评价能够快速地反映出一个机器翻译的质量好还是不好，相比人工评价而言，自动评价成本低、效率高。</p>
<p>现在一般采用的方法是，基于n-gram（n元语法）的评价方法。通常大家都用BLEU值。一般地，BLEU是在多个句子构成的集合（测试集）上计算出来的。这个测试集可能包含一千个句子或者两千个句子，去整体上衡量机器翻译系统好还是不好。有了这个测试集以后，需要有参考答案（reference）。所谓参考答案就是人类专家给出的译文。这个过程很像考试，通过比较参考答案和系统译文的匹配程度，来给机器翻译系统打分。</p>
<p>为了简便，此处我们用一个句子进行说明。比如说就这个句子而言，reference是『I read a book on Sunday』。那么上图中有两个系统译文，一个是system1 ，一个是system2。显见，system2的得分会更高，因为它的译文跟reference是完全匹配的，system1匹配了一些片段，但是不连续。在计算BLEU得分的时候，连续匹配的词越多，得分越高。</p>
<p>当然，BLEU值也有比较明显的缺点。用一个词来举例，比如『你好』，人给出的一个参考译文是『hello』。机器给出的译文是『how are you』，跟这个reference没有一个词匹配上，从BLEU值的角度来看，它得分是零。但是你能说它错吗？它翻译的很好。所以BLEU值的得分，受reference影响。Reference越多样化，匹配上的可能性就会越大。一般来说，用于评价机器翻译质量的测试集有4个reference，也有的有一个reference，也有的有十几个reference。BLEU分数受测试领域、reference多样性等多种因素的影响，抛开具体的设置，单说一个分数不具有参考性。</p>
<p>基于同一个测试集，针对不同的翻译系统结果，可以依据上述公式计算BLEU值，从而快速比较多个翻译系统的好坏。通常国际评测中，同时采用自动评价和人工评价方法衡量参赛系统。</p>
<p><img src="https://pic2.zhimg.com/v2-01cce5b9aaa3a77641419c7f591f7d0d\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="623" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic2.zhimg.com/v2-01cce5b9aaa3a77641419c7f591f7d0d_r.jpg"></p>
<p>这张图显示了近年来机器翻译质量的进步。这个BLEU值是在5个reference上计算出来的，衡量我们中英翻译的质量。2014年的时候，我们用的还是统计机器翻译的方法。从2015年到现在，随着神经网络翻译方法的不断进步，翻译质量一直是持续提高的。通常来说，BLEU值提高1个百分点就是非常显著的提高。在统计机器翻译时代，每年BLEU提高1个百分点都是比较大的挑战。而在神经网络翻译上线后的这四年之间，我们大约每年都有5、6个百分点BLEU值的提升。</p>
<p><img src="https://pic3.zhimg.com/v2-2a2fa759ea3a8604a0e1e87341242dd6\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="591" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic3.zhimg.com/v2-2a2fa759ea3a8604a0e1e87341242dd6_r.jpg"></p>
<p>我们通过一个例子，直观的感受一下神经网络方法的译文质量。这个例子是某一年的大学英语六级考试的翻译真题。这个例子我飘了不同的颜色，表示两种语言句子成分的对应关系。从颜色上我们可以看出来，与原文相比，译文的词语顺序发生了比较大变化。比如说，中文句子中的『尽快』, 在英语端，『as soon as possible』换到后面去了，进行了比较长距离的调序。这在统计机器翻译时代是非常难做的事情，但是神经网络翻译能够把它处理的很好。</p>
<p><img src="https://pic4.zhimg.com/v2-0ff6dbb64fcdb08d56a7d117b3954973\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="597" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic4.zhimg.com/v2-0ff6dbb64fcdb08d56a7d117b3954973_r.jpg"></p>
<p>刚才说的它包含编码器和解码器，先来看编码器。它进行了一个双向的编码，双向的编码干了一个什么事情？就是把词用词向量来表示。那么如何做到这一点呢？我们首先有一个词向量表，是通过神经网络训练出来的。源语言句子中的词，可以用一个one hot的向量表示。所谓one hot就是，比如上例中中文句子有8个词。哪个词出现了，就把这个词标为1，其他的词标为0。比如第4个词“看”这个词是1，那么其他的都是0。这两个矩阵这么一乘，相当于一个查表的操作。就把其中这个词向量表的一列取出来了，那么这一列的向量就代表了这个词。神经网络里面所有的词都会用向量来表示。得到词的向量表示后，再经过一个循环神经网络的变换，得到另外一个向量，称为Hidden State（隐状态）。</p>
<p>为什么做了一个双向的编码？是为了充分利用上下文信息。比如说，如果只是从左往右编码，“我在周日看”，看的是什么呢？“看”后面的你不知道，因为你只得到了“看”前面的信息。那么怎么知道后面的信息呢，这时候我们就想那能不能从后面到前面再进行一个编码，那就是“书本一了看”，从后面往前的编码，这时候“看”呢既有前面的信息，也有后面的信息。所以它有了一个上下文的信息，可以进一步提高译文质量。</p>
<p>刚才提到，每个词经过一系列变换，映射为一个向量表示。如果将双向编码的向量结合起来呢? 现在一般采用一个非常简单的方法，将两个向量进行拼接。比如两个256维的向量，拼接完成后得到一个512维的向量，用来表示一个词。</p>
<p><img src="https://pic1.zhimg.com/v2-98d6a9664e8f0b9920c8ce497493ea94\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="599" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic1.zhimg.com/v2-98d6a9664e8f0b9920c8ce497493ea94_r.jpg"></p>
<p>编码完成以后，需要把这个源语言的句子压缩到一个向量里去。这一步是怎么做的？一个最简单的方式是把这所有的向量加起来。但是后来大家发现这样其实不太合理。为什么不太合理，因为每一个词都是被作为相同的权重去对待的，那显然是不合理的，这时候就提出了一个注意力机制，叫Attention。这里用不同深度颜色的线去表示Attention的能量强弱，用以衡量产生目标词时，它所对应的源语言词的贡献大小。所以呢h前面又加一个α，α就表示它的一个权重。</p>
<p><img src="https://pic1.zhimg.com/v2-c5585f838173b32985730d327996bc64\_b.gif" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="357" data-thumbnail="https://pic1.zhimg.com/v2-c5585f838173b32985730d327996bc64\_b.jpg" class="origin\_image zh-lightbox-thumb" width="640" data-original="https://pic1.zhimg.com/v2-c5585f838173b32985730d327996bc64\_r.jpg"></p>
<p><img src="https://pic1.zhimg.com/v2-c5585f838173b32985730d327996bc64_b.jpg" alt=""></p>
<p>有了句子的向量表示后，就掌握了整个源语言句子的所有的信息。解码器就开始从左到右一个词一个词的产生目标句子。在产生某个词的时候，考虑了历史状态。第一个词产生以后，再产生第二个词，直到产生句子结束符EOS(End of Sentence) ，这个句子就生成完毕了。  </p>
<p><img src="https://pic3.zhimg.com/v2-578eff847c1f7303a590d0db4caa4b36\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="605" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic3.zhimg.com/v2-578eff847c1f7303a590d0db4caa4b36_r.jpg"></p>
<p>去年以来出现了一个大杀器，TRANSFORMER。基本上取得了目前来说神经网络机器翻译最好的效果。TRANSFORMER的改进在哪里，来源一篇谷歌的论文，叫“Attention Is All You Need”。上文提到有一个注意力机制，这篇论文所提出的方法，可以只用注意力机制就把翻译搞定了。</p>
<p><img src="https://pic2.zhimg.com/v2-abc1cddace800cfdaa683c3c817b2a85\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="600" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic2.zhimg.com/v2-abc1cddace800cfdaa683c3c817b2a85_r.jpg"></p>
<p>那么它是怎么来做的呢？它其实也有一个编码器和一个解码器，这个是架构是没有变的。其中编码器和解码器都有多层。下面我们通过一个具体例子，来简单解释一下其原理。</p>
<p><img src="https://pic1.zhimg.com/v2-f7dd5a4d85cde44515a4a989bb6d83d8\_b.gif" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="357" data-thumbnail="https://pic1.zhimg.com/v2-f7dd5a4d85cde44515a4a989bb6d83d8\_b.jpg" class="origin\_image zh-lightbox-thumb" width="640" data-original="https://pic1.zhimg.com/v2-f7dd5a4d85cde44515a4a989bb6d83d8\_r.jpg"></p>
<p><img src="https://pic1.zhimg.com/v2-f7dd5a4d85cde44515a4a989bb6d83d8_b.jpg" alt=""></p>
<p>我们这个句子就包含两个词 『看书』。</p>
<p>论文中，把每一个词都用三个向量表示，一个叫Query（Q），一个叫Key（K），另外一个是Value（V）。那怎么得到一个词的Query、Key和Value呢？左边有三个矩阵，WQ、WK和WV，只要跟每一词向量相乘，就能够把这个词转换成三个向量表示。那么目标是什么，我们想把『看』这样一个词，通过一系列的网络变换，抽象到高维的向量表示。</p>
<p>通过Q和K进行点积，并通过softmax得到每个词的一个attention权重，在句子内部做了一个attention，称作Self Attention。Self Attention可以刻画句子内部各成分之间的联系，比如说“看”跟“书”之间就建立了联系。这样，每个词的向量表示（Z）就包含了句子里其他词的关联信息。</p>
<p><img src="https://pic4.zhimg.com/v2-0425bdc29df51560c25ddebf7817b9fb\_b.gif" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="357" data-thumbnail="https://pic4.zhimg.com/v2-0425bdc29df51560c25ddebf7817b9fb\_b.jpg" class="origin\_image zh-lightbox-thumb" width="640" data-original="https://pic4.zhimg.com/v2-0425bdc29df51560c25ddebf7817b9fb\_r.jpg"></p>
<p><img src="https://pic4.zhimg.com/v2-0425bdc29df51560c25ddebf7817b9fb_b.jpg" alt=""></p>
<p>作者认为只有这一个QKV不太够，需要从多个角度去刻画。如何做呢？提出了“Multi-head”。在里面论文里面定义了8组QKV的矩阵，当然也可以定义16个，这个数值可以自定义。在通过一系列变换，最终得到了每个词的向量表示。这只是encoder一层。那么这一层的输出做为下一层的输入，再来一轮这样的表示，就是Encoder-2，那么再来一轮就是第三层，如此一直到第N层。Decoder也是类似，不再解释。感兴趣的可以阅读原文。</p>
<p><strong>技术挑战</strong></p>
<p>尽管神经网络带来了翻译质量的巨大提升，然而仍然面临许多挑战。</p>
<p><img src="https://pic3.zhimg.com/v2-0ee541bfeebc736e19d8f593fcda8096\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="609" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic3.zhimg.com/v2-0ee541bfeebc736e19d8f593fcda8096_r.jpg"></p>
<p>第一个挑战就是漏译，很多时候，原语言句子有些词没有被翻译出来，比如说在这个句子里面，『假』和『恶』没有被翻译出来。甚至有的时候输入一个长句子有逗号分隔，有几个子句都没有翻译出来。这确实是神经网络翻译面临的一个问题。通过刚才的讲解知道，翻译模型把原文句子整体读进去以后形成了一个向量，然后再对这个向量进行解码。翻译模型认为有些词不应该产生，从而漏掉了译文。</p>
<p>漏译的原因是什么，如何解决这个问题？这方面有很多工作，下面我就从几个方面去讲一下。我们今年有一篇论文从数据方面去分析。我们发现漏译与词语的熵成正相关关系，这个词的熵越大，漏译的可能性越大。它所对应的目标语言词越多，概率越分散（熵越大），越有可能被漏译。</p>
<p><img src="https://pic2.zhimg.com/v2-27d356b60af479a2f16e154b55fbe10d\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="599" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic2.zhimg.com/v2-27d356b60af479a2f16e154b55fbe10d_r.jpg"></p>
<p>左边的例子，S1对应3种不同的翻译，（s1,t1) (s1,t2) (s1, t3 t4)，它的熵就比较大。我们把所有对应的翻译统一替换为一个特殊词『stoken4s1』，以降低词语翻译的熵值。右边呢是我们提出来的三种方法，去改善翻译结果，包括pre-training, multitask learning, two-pass decoding。大家有兴趣的话，可以去看论文。</p>
<p><img src="https://pic3.zhimg.com/v2-740b6b55a3cf9e88ec7fdc46b9b13496\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="605" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic3.zhimg.com/v2-740b6b55a3cf9e88ec7fdc46b9b13496_r.jpg"></p>
<p>从实验结果来看，相比Transformer，在中英翻译质量上有显著提高，高熵值词语的漏译比例显著下降。</p>
<p><img src="https://pic1.zhimg.com/v2-923fdb64b8fc8fe234c9b33e623ad640\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="606" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic1.zhimg.com/v2-923fdb64b8fc8fe234c9b33e623ad640_r.jpg"></p>
<p>第二个挑战就是数据稀疏。相比于统计机器翻译，这个问题对神经网络翻译而言，更严重。实验表明，神经网络对于数据量更敏感。</p>
<p><img src="https://pic4.zhimg.com/v2-c2d3ac6fc1bdea882884f5821df509e7\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="600" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic4.zhimg.com/v2-c2d3ac6fc1bdea882884f5821df509e7_r.jpg"></p>
<p>针对数据稀疏问题，我们提出了一个多任务学习的多语言翻译模型。在进行多语言翻译的时候，源语言共享编码器，在解码端，不同的语言，使用不同的解码器。这样在源语言端就会共享编码器的信息，从而缓解数据稀疏问题。后来，加拿大蒙特利尔大学、Google等在此方向上陆续开展了多个工作。</p>
<p><img src="https://pic1.zhimg.com/v2-355f982a017a08413d302c45e52adab4\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="606" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic1.zhimg.com/v2-355f982a017a08413d302c45e52adab4_r.jpg"></p>
<p>实验表明，我们的方法收敛更快，翻译质量也明显提高。更多细节，请阅读论文。</p>
<p><img src="https://pic1.zhimg.com/v2-6fa9a34d85ef8eec47a48cae5a916634\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="609" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic1.zhimg.com/v2-6fa9a34d85ef8eec47a48cae5a916634_r.jpg"></p>
<p>这篇论文是2018年EMNLP上的best paper，提出了一个统一的框架。A)里面蓝色的点和红色的点分别代表两种不同的语言句子。如何通过两种语言的单语数据构建翻译系统呢？</p>
<p>首先我要做一个初始化，B）是初始化。首先构建一个词典，把这两种语言之间的词做一下对齐。C）是语言模型，基于单语数据，可以训练语言模型，用来衡量这个语言的流利度。那么D）是什么? D）是一个称作Back Translation的技术，是目前大家常用的一个用于增强数据的方法。</p>
<p>用B）初始化后构建的一个词典，就可以从一种语言翻译为另外一种语言，哪怕是先基于词的翻译。然后，用另外一种语言的语言模型去对译文进行衡量。然后把得分高的句子挑出来，再翻译回去，这一过程称作Back Translation，然后再用原来那种语言的语言模型去衡量这个句子好还是不好。这样一轮一轮的迭代，数据就会变得越来越好，系统翻译质量也会越来越好。</p>
<p>第三个挑战就是引入知识，如何将更多丰富的知识引入翻译模型是机器翻译长期面临的挑战。这个例子中，中文句子中『横流』对应到目标语言端是没有翻译出来的，用一个特殊的记号叫UNK（Unknown Word）来标记。</p>
<p><img src="https://pic1.zhimg.com/v2-4b61c8fc4267a708a971154c25af8470\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="599" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic1.zhimg.com/v2-4b61c8fc4267a708a971154c25af8470_r.jpg"></p>
<p>那么我们做一个什么样的工作呢？我们引入了几种知识，第一种就是叫短语表或者叫词表。如果发现『横流』这个词没有被翻译出来，我们就去查这个词典，这个词典就作为一个外部知识被引入进来了。同时，那我们还引入了一个语言模型，语言模型去衡量目标语言的这个句子是不是流畅。同时，我们引入一个长度奖励特征去奖励长句子。因为句子越长，可能漏掉的信息就越少。这个工作首次将统计机器翻译中的特征引入神经网络翻译，可以作为引入知识的一个框架。</p>
<p><img src="https://pic3.zhimg.com/v2-72cdceae6218e5195421de479fefab76\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="597" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic3.zhimg.com/v2-72cdceae6218e5195421de479fefab76_r.jpg"></p>
<p>但是目前来说，引入知识还是比较表层的。知识的引入，还需要更多更深入的工作。比如说这个例子， 这个句子是存在歧义的。『中巴』 在没有给上下文的时候，是无法判断『巴』是哪个国家的简称。</p>
<p>但是下面的句子，有一个限定，“金砖框架”。这个时候，人们就知道该如何翻译了。但是，机器能不能知道？大家可以去翻译引擎上去验证。因为人是知道中国跟哪些国家是金砖国家，但是机器没有这个知识。怎么把这个知识交给机器去做，这是一个非常挑战的问题。</p>
<p><img src="https://pic2.zhimg.com/v2-5988edf384c0e9455e30fc736a700301\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="602" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic2.zhimg.com/v2-5988edf384c0e9455e30fc736a700301_r.jpg"></p>
<p>还有一个挑战，是可解释性：神经网络翻译到底是神还是神经？虽然人们可以设计和调整网络结构，去优化系统，提高质量。但是对于该方法还缺乏深入的理解。</p>
<p>也有很多工作去试图研究网络内部工作机理。清华大学有一篇文章从注意力的角度去进行研究。</p>
<p><img src="https://pic2.zhimg.com/v2-f5b0e532b1ba58cf937aa4bf88148771\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="603" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic2.zhimg.com/v2-f5b0e532b1ba58cf937aa4bf88148771_r.jpg"></p>
<p>比如左边的例子，出现了一个UNK，那个UNK是怎么产生的，它虽然没有被翻译出来，但是出现在正确的位置，占了一个位置。通过Attention对应关系，可以看到这个UNK对应到『债务国』。右边例子是一个重复翻译的现象。神经网络机器翻译除了经常漏翻译之外，还会经常重复翻译。比如说出现了两个“history”。那么通过这个对应关系我们就可以看到，第6个位置上的“history”是重复出现的，它的出现不仅跟第一个位置“美国人”和第二个位置“历史”相关，还跟第5个位置“the”相关。因为产生了一个定冠词“the”，模型认为这个地方应该出现一个“history”，这篇文章对这样的例子进行了大量的分析，并且给出了一些分析结果和解决方案。如需进一步了解，可以看原始论文。</p>
<p>还有第五个挑战 ，是机器翻译长期以来面临的挑战，语篇翻译。大部分的翻译系统现在所使用的翻译方法都是基于句子，以句子作为单位，一个句子一个句子的进行翻译。单看这三个句子翻译还可以接受。但是连起来看就觉得生硬不连贯。  </p>
<p><img src="https://pic1.zhimg.com/v2-59708dbcab70ace657847a68eb5b0304\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="600" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic1.zhimg.com/v2-59708dbcab70ace657847a68eb5b0304_r.jpg"></p>
<p>我们的方法输出的结果。可以看到，定冠词、代词的加入提升了句子间的连贯性。</p>
<p><img src="https://pic3.zhimg.com/v2-0f33018f35c75e62d0e3b62598c11c22\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="607" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic3.zhimg.com/v2-0f33018f35c75e62d0e3b62598c11c22_r.jpg"></p>
<p><img src="https://pic3.zhimg.com/v2-f915521420f0ad589946eba80cacf5e6\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="603" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic3.zhimg.com/v2-f915521420f0ad589946eba80cacf5e6_r.jpg"></p>
<p>我们提出了一个两步解码的方法。在第一轮解码中单独生成每个句子的初步翻译结果，在第二轮解码中利用第一轮翻译的结果进行翻译内容润色，并且提出使用增强式学习模型来奖励模型产生更流畅的译文。这是我们系统输出的一个结果，整体上，流畅度提高了。 具体细节，可以去看论文。</p>
<p><img src="https://pic2.zhimg.com/v2-e9eb09ee335deb9284ff6b3a2a25c445\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="600" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic2.zhimg.com/v2-e9eb09ee335deb9284ff6b3a2a25c445_r.jpg"></p>
<p>前面我们讲了机器翻译的原理以及神经网络翻译的发展、以及面临的挑战，我们现在看一看，机器翻译现在有哪些应用？</p>
<p>机器翻译在越来越多地帮助和影响我们的生活。上图中列出了几个利用机器翻译来进行日常学习和交流的例子。左边两位是明星，一位是电影明星，利用翻译系统翻译化妆品成分，一位是公交司机，利用翻译APP学习英文，并运用到工作中去。右边是交警、售票员利用机器翻译与外国人进行交流。</p>
<p>这只是机器翻译应用的一个缩影。随着技术的发展和交流的需要，机器翻译已经深切地融入我们的生活。</p>
<p><img src="https://pic4.zhimg.com/v2-5ddd5545986d67d895663f97c9ff2f83\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="608" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic4.zhimg.com/v2-5ddd5545986d67d895663f97c9ff2f83_r.jpg"></p>
<p>机器翻译除了能做翻译之外还可以做一些很有意思的事情，比如说白话文和文言文的翻译，其实用的技术是类似的，从一种语言表达翻译成另外一种语言表达，或者从一个字串变换为另外一个字串，甚至是从一幅图产生文字说明，凡是这样的工作都可以用机器翻译的技术来做。</p>
<p><img src="https://pic2.zhimg.com/v2-dd24d302ec255dc851f30f5742eb4f95\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="608" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic2.zhimg.com/v2-dd24d302ec255dc851f30f5742eb4f95_r.jpg"></p>
<p>机器翻译还可以做什么，作诗。这是机器翻译写的两首诗，大家可以看一下，右边这个，“腊梅开时独自寒，幽香一缕为哪般。东风不解相思意，吹落梨花雪满天。”意境是不错的。而它的原理也是机器翻译的原理，从上一句翻译到下一句，可以认为上一句是原文，下一句是目标译文，然后再把第一句和第二句作为原文，产生第三句，这样整首诗每一句的产生都是依赖于前面的信息，所以整体上就有一个语义上的连贯性。</p>
<p><img src="https://pic2.zhimg.com/v2-af4122d543eaaaa1372d3cdd74c068a5\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="601" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic2.zhimg.com/v2-af4122d543eaaaa1372d3cdd74c068a5_r.jpg"></p>
<p>这是利用机器翻译做诗的原理图。首先根据用户输入提取关键词，并对关键词进行联想和扩展，继而产生意境一致、连贯的诗句。</p>
<p><img src="https://pic1.zhimg.com/v2-3f6983204f4c356ca24246244269ea80\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="612" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic1.zhimg.com/v2-3f6983204f4c356ca24246244269ea80_r.jpg"></p>
<p>机器翻译还可以做什么，写春联。</p>
<p>这个对联跟以往常见的对联不太一样的地方，是我们上联、下联和横批全部都是机器产生的。以前的对对子，我出一个上联，你对一个下联儿，但是春联这个有意思，用户提供一个词语，机器自动做出上下联和横批。 这里左边两个是人名，第一个是云鹏，分别出现在上下联中第4个字的位置。第二个是黄渤，两个字分别出现在开头。最后这个，上联中前两个字是一个电影的名字。 可见机器翻译有技术很多有趣的应用。</p>
<p><img src="https://pic4.zhimg.com/v2-60a3e9d76358b3d92011314792f433cf\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="609" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic4.zhimg.com/v2-60a3e9d76358b3d92011314792f433cf_r.jpg"></p>
<p>现在语音、图像等人工智能技术不断发展，并取得了很大的进步，那么能不能跟机器翻译结合起来，创造出更多有意思的应用和场景呢?</p>
<p><img src="https://pic3.zhimg.com/v2-56538dd438aa6b8b92052c8c5e2aec52\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="582" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic3.zhimg.com/v2-56538dd438aa6b8b92052c8c5e2aec52_r.jpg"></p>
<p>第一个就是同声传译，我把这四个字拆开来分析。『同』，就是表示时间延迟要短，为什么同传比交传的工资高，就是因为它体现在这个『同』上，在我说话的同时，基本上翻译结果就传递到观众那边去了；『声』用到的是语音技术，包括语音识别和合成；『传』就是信息传递要准确，翻译的时候得把我原本的意思表达出来；『译』就是翻译技术，对应到机器翻译。正好这四个字包含了两个要求、两个技术。</p>
<p>那么挑战在什么地方？我们来看下图中的这个句子，这是一个语音识别的结果，那么要把这样一个语音识别的结果去做同声传译，用机器把它翻译出来，有哪些问题呢？</p>
<p><img src="https://pic2.zhimg.com/v2-69e70cd0e85369c34e67d6cf84d54f5d\_b.gif" data-caption="" data-size="normal" data-rawwidth="593" data-rawheight="304" data-thumbnail="https://pic2.zhimg.com/v2-69e70cd0e85369c34e67d6cf84d54f5d\_b.jpg" class="origin\_image zh-lightbox-thumb" width="593" data-original="https://pic2.zhimg.com/v2-69e70cd0e85369c34e67d6cf84d54f5d\_r.jpg"></p>
<p><img src="https://pic2.zhimg.com/v2-69e70cd0e85369c34e67d6cf84d54f5d_b.jpg" alt=""></p>
<p>第一个问题就是有噪声的问题。比如说有冗余，我现在说话的时候可能就有冗余，很多词你是不需要翻出来的，或者是我的口语，或者是我有重复。另外一个就是识别错误，这里面『种庄稼』识别错了，人可能纠错能力会更强，能知道是种庄稼。但是这对机器很难，这种错误直接影响翻译质量，所以就需要建立一个更鲁棒的语音模型和翻译模型去做更好的容错。</p>
<p>第二个难点是句读、断句和标点，刚才大家看了是没有标点的（上图），没有标点的时候你不知道该在哪里翻，该在哪里停顿，所以我们应该给他加上标点，那这个问题可以看做一个序列标注问题来解决它。</p>
<p>第三个难点是时延，时延其实跟准确率是一个矛盾的概念，人们说话是有逻辑的，要想翻译的准，我就可能得等到语义表达相对完整了再翻，但是那个时候很有可能就已经延迟了很长时间了，那这个时候那应该怎么做？我们可以采取一个适当的预测技术，得到一个翻译质量和时延的平衡。</p>
<p><img src="https://pic4.zhimg.com/v2-45b92134413ec07fb3219db941b1ddf7\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="492" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic4.zhimg.com/v2-45b92134413ec07fb3219db941b1ddf7_r.jpg"></p>
<p>另外一个应用是翻译机。除了像刚才说的会议级别的同传需求以外，另一个满足我们日常交流的需求，我们出国的时候跟外国人可以自由交流。</p>
<p>出国面临几个痛点，一是上网，二是翻译。我们这款翻译机同时解决了这两个问题，你可以用它上网，可以用它来翻译，还可以用它来导览。</p>
<p>从网上用户公开的反馈来看，翻译机在很多场景下对用户帮助都非常大。比如说在乘车、在吃饭的时候，甚至是买药。在买药的例子中，这个人本身他是懂英文的，但是药品的名字他不会，那他就用翻译机把那个药的名字翻译出来。下面的例子，在酒店check in的时候，过安检的时候，翻译机都可以很好的帮助人们进行交流。</p>
<p><img src="https://pic4.zhimg.com/v2-57f99be6ec49fa41d6cf77036038b85b\_b.gif" data-caption="" data-size="normal" data-rawwidth="782" data-rawheight="417" data-thumbnail="https://pic4.zhimg.com/v2-57f99be6ec49fa41d6cf77036038b85b\_b.jpg" class="origin\_image zh-lightbox-thumb" width="782" data-original="https://pic4.zhimg.com/v2-57f99be6ec49fa41d6cf77036038b85b\_r.jpg"></p>
<p><img src="https://pic4.zhimg.com/v2-57f99be6ec49fa41d6cf77036038b85b_b.jpg" alt=""></p>
<p>下面一个应用是我非常喜欢的，就是图像翻译。我去国外的时候有时会去博物馆，我不太愿意看大段的介绍说明，因为那上面的字很难看懂。有了这个功能以后，我去博物馆里，每个展品下面有一个说明，那么我可以去拍一下，翻译为母语进行阅读。  </p>
<p>上图中的右边是一个实物翻译的功能，识别物品，并同时给出中英文翻译，可以帮助我们进行双语学习。</p>
<p><img src="https://pic3.zhimg.com/v2-7f7b4fdeeec5ce68652111d19ec5cf8a\_b.gif" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="329" data-thumbnail="https://pic3.zhimg.com/v2-7f7b4fdeeec5ce68652111d19ec5cf8a\_b.jpg" class="origin\_image zh-lightbox-thumb" width="640" data-original="https://pic3.zhimg.com/v2-7f7b4fdeeec5ce68652111d19ec5cf8a\_r.jpg"></p>
<p><img src="https://pic3.zhimg.com/v2-7f7b4fdeeec5ce68652111d19ec5cf8a_b.jpg" alt=""></p>
<p>最后，我来总结一下，我画了一个机器翻译的立方体，用三个维度去表征现在机器翻译的发展，试图衡量一下现在机器翻译和人的翻译都处在一个什么水平。这三个维度，一个是翻译质量，一个是领域，还有一个是语言种类，我们最终的目标是要右上角这个顶端，我们在所有的语言、所有的领域上都达到一个非常高的翻译质量。</p>
<p>那么人就是这个平面，我把它称为专家平面，一个人是某一个领域的专家，可能是某一种语言的专家，比如说我是中英、化学领域的专家，那么我就可以把化学领域、中英这两种语言翻译的很好。但是让我翻译中文到韩文，翻译医药领域，我不懂这种语言、不是这个领域的专家，就翻译的不太好。但是好在有别的专家，有的人可能懂好几种语言，或者跨了好几个领域，所以整个人类专家是分布在这个平面上的。</p>
<p>机器理论上来说，它可以做任何语言和任何领域的翻译。但是它的翻译质量显然是不如人好，但是在某些特定的领域上我们可以进行一些领域的定制化或者领域的优化，它可以在某一个领域上往前推进，所以机器翻译最终的目标是达到终级目标，当然这个路非常漫长。</p>
<p><img src="https://pic1.zhimg.com/v2-14e1045e08318a8d8f6a3755be4e5060\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="608" class="origin\_image zh-lightbox-thumb" width="1080" data-original="https://pic1.zhimg.com/v2-14e1045e08318a8d8f6a3755be4e5060_r.jpg"></p>
<p>50多年前，也就是在1964年的时候，有三位科学家写了一本书叫《机器翻译浅说》，提出了机器翻译的几个设想。</p>
<p>第一个设想，『有一天，当你在人民大会堂的时候，你会发现无论哪个国家的人在台上讲话，与会者都能从耳机里听到自己国家的语言。同时你会发现耳机里翻译的不是人，而是我们的万能翻译博士』。这其实就是自动同传。</p>
<p>第二个设想，『此外，当你去国外旅行的时候，随身可以带一个半导体和其他材料制成的小型万能博士。当我们跟国外，外国朋友交谈的时候，博士就立刻给你翻译出各自国家的语言』。这就是我们刚才讲的翻译机。</p>
<p>五十多年前的预言，其实现在已经出现在我们身边了。当然还有一个预言，现在还没有看到产品，就是翻译打印机。比如说有一天在英国出版了一本新书，你把它放在这个打印机里去，那么出来的就是已经译好的中文译本、德文译本、俄文译本。</p>
<p>其实机器翻译想做的事情，就是想让人们在任何时间、任何地点，用任何语言可以进行自由的沟通。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/12/14/recommender_system/" rel="next" title="个性化推荐">
                <i class="fa fa-chevron-left"></i> 个性化推荐
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/03/22/Attention Is All You Need（Transformer）算法原理解析/" rel="prev" title="Attention Is All You Need（Transformer）算法原理解析">
                Attention Is All You Need（Transformer）算法原理解析 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/headpic.png" alt="Yimitri">
            
              <p class="site-author-name" itemprop="name">Yimitri</p>
              <p class="site-description motion-element" itemprop="description">深度学习 网络安全</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">18</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">分类</span>
                
              </div>
            

            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/newdimitri" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:Yimitrii@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://ws3.sinaimg.cn/large/005BYqpgly1g1jru8map2j30kw0r241d.jpg" target="_blank" title="Wechat">
                      
                        <i class="fa fa-fw fa-weixin"></i>Wechat</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.zhihu.com/org/peddlepaddleqian-yan-gen-zong/activities" target="_blank" title="知乎">
                      
                        <i class="fa fa-fw fa-mortar-board (alias)"></i>知乎</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yimitri</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">59.1k</span>
  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>



<!-- 新增访客统计代码 -->

<div class="busuanzi-count">
    <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="site-uv">
      <i class="fa fa-user"></i>
      访问用户： <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 人
    </span>
    <div class="powered-by"></div>
    <span class="site-uv">
      <i class="fa fa-eye"></i>
      访问次数： <span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 次
    </span>
    <!-- 博客字数统计 -->
    <span class="site-pv">
      <i class="fa fa-pencil"></i>
      博客全站共： <span class="post-count">59.1k</span> 字
    </span>
</div>
<!-- 新增访客统计代码 END-->
        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://于祥.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2019/03/21/机器翻译从古至今全追踪/';
          this.page.identifier = '2019/03/21/机器翻译从古至今全追踪/';
          this.page.title = '机器翻译从古至今全追踪';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://于祥.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  

  

  

</body>
</html>
